{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "svm.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiragbaid7/cs231n/blob/master/assignment1/svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-title"
        ],
        "id": "pJggAVxYGD03",
        "colab_type": "text"
      },
      "source": [
        "# Multiclass Support Vector Machine exercise\n",
        "\n",
        "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
        "\n",
        "In this exercise you will:\n",
        "    \n",
        "- implement a fully-vectorized **loss function** for the SVM\n",
        "- implement the fully-vectorized expression for its **analytic gradient**\n",
        "- **check your implementation** using numerical gradient\n",
        "- use a validation set to **tune the learning rate and regularization** strength\n",
        "- **optimize** the loss function with **SGD**\n",
        "- **visualize** the final learned weights\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq3sKbXWGuoQ",
        "colab_type": "code",
        "outputId": "3935a54a-46ad-47f8-8da9-7b85c1eb6851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBnPKtxSGyH-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "2749f61b-7f06-415a-8aca-75d2164e3f07"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/assignment1/cs231n/datasets\n",
        "!chmod +x get_datasets.sh\n",
        "!./get_datasets.sh\n",
        "\n",
        "import sys\n",
        "%cd /content/drive/My\\ Drive/assignment1\n",
        "sys.path.append('/content/gdrive/My Drive/assignment1')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/assignment1/cs231n/datasets\n",
            "--2019-12-25 16:10:45--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  30.3MB/s    in 6.1s    \n",
            "\n",
            "2019-12-25 16:10:51 (26.8 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n",
            "/content/drive/My Drive/assignment1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "12Mh7WjkGD08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run some setup code for this notebook.\n",
        "import random\n",
        "import numpy as np\n",
        "from cs231n.data_utils import load_CIFAR10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# This is a bit of magic to make matplotlib figures appear inline in the\n",
        "# notebook rather than in a new window.\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# Some more magic so that the notebook will reload external python modules;\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "wAsIyJJOGD1E",
        "colab_type": "text"
      },
      "source": [
        "## CIFAR-10 Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "nus2Yq0IGD1I",
        "colab_type": "code",
        "outputId": "40a7e2db-abfb-416f-b3ce-da0269a71fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Load the raw CIFAR-10 data.\n",
        "cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
        "\n",
        "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
        "try:\n",
        "   del X_train, y_train\n",
        "   del X_test, y_test\n",
        "   print('Clear previously loaded data.')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "\n",
        "# As a sanity check, we print out the size of the training and test data.\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape:  (50000, 32, 32, 3)\n",
            "Training labels shape:  (50000,)\n",
            "Test data shape:  (10000, 32, 32, 3)\n",
            "Test labels shape:  (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "o3k-UayWGD1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize some examples from the dataset.\n",
        "# We show a few examples of training images from each class.\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "num_classes = len(classes)\n",
        "samples_per_class = 7\n",
        "for y, cls in enumerate(classes):\n",
        "    idxs = np.flatnonzero(y_train == y)\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt_idx = i * num_classes + y + 1\n",
        "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "        plt.imshow(X_train[idx].astype('uint8'))\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(cls)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "E7e9_lROGD1R",
        "colab_type": "code",
        "outputId": "74cfc489-18d8-44fb-bbd4-a389d4643a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Split the data into train, val, and test sets. In addition we will\n",
        "# create a small development set as a subset of the training data;\n",
        "# we can use this for development so our code runs faster.\n",
        "num_training = 49000\n",
        "num_validation = 1000\n",
        "num_test = 1000\n",
        "num_dev = 500\n",
        "\n",
        "# Our validation set will be num_validation points from the original\n",
        "# training set.\n",
        "mask = range(num_training, num_training + num_validation)\n",
        "X_val = X_train[mask]\n",
        "y_val = y_train[mask]\n",
        "\n",
        "# Our training set will be the first num_train points from the original\n",
        "# training set.\n",
        "mask = range(num_training)\n",
        "X_train = X_train[mask]\n",
        "y_train = y_train[mask]\n",
        "\n",
        "# We will also make a development set, which is a small subset of\n",
        "# the training set.\n",
        "mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "X_dev = X_train[mask]\n",
        "y_dev = y_train[mask]\n",
        "\n",
        "# We use the first num_test points of the original test set as our\n",
        "# test set.\n",
        "mask = range(num_test)\n",
        "X_test = X_test[mask]\n",
        "y_test = y_test[mask]\n",
        "\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape:  (49000, 32, 32, 3)\n",
            "Train labels shape:  (49000,)\n",
            "Validation data shape:  (1000, 32, 32, 3)\n",
            "Validation labels shape:  (1000,)\n",
            "Test data shape:  (1000, 32, 32, 3)\n",
            "Test labels shape:  (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "yE9sKY3-GD1W",
        "colab_type": "code",
        "outputId": "00e582a6-b1a7-42e8-9a1f-86c3300115ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Preprocessing: reshape the image data into rows\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "\n",
        "# As a sanity check, print out the shapes of the data\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('dev data shape: ', X_dev.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape:  (49000, 3072)\n",
            "Validation data shape:  (1000, 3072)\n",
            "Test data shape:  (1000, 3072)\n",
            "dev data shape:  (500, 3072)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore-input"
        ],
        "id": "tZ10yrbGGD1d",
        "colab_type": "code",
        "outputId": "1a920dd2-1cdd-47ab-be6a-7bafbbe5c81b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "# Preprocessing: subtract the mean image\n",
        "# first: compute the image mean based on the training data\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "print(mean_image[:10]) # print a few of the elements\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image\n",
        "plt.show()\n",
        "\n",
        "# second: subtract the mean image from train and test data\n",
        "X_train -= mean_image\n",
        "X_val -= mean_image\n",
        "X_test -= mean_image\n",
        "X_dev -= mean_image\n",
        "\n",
        "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
        "# only has to worry about optimizing a single weight matrix W.\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[130.64189796 135.98173469 132.47391837 130.05569388 135.34804082\n",
            " 131.75402041 130.96055102 136.14328571 132.47636735 131.48467347]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAR/ElEQVR4nO3db6hl5XXH8e+K0cR7FUdrOgyjVGOF\nIqEZ5TJYIsEmJFgJqFBEX4gvJJO2ESqkL8RCtdAXplRFaDGMdcikWP80Kg5F2pghIHljvFodR6dt\njIzEYZwxqGjnhqbjrL7Ye+COnOc556yz9z5H1+8Dwz137/PsZ909Z919zl73eR5zd0Tkk+9T8w5A\nRIahZBdJQskukoSSXSQJJbtIEkp2kSQ+PUtjM7sCuBc4CfhHd7+z9vzl5WXfcOaGWbocgE3fYvom\nMmfxivNil6rfe/c9jhw5MvIVGU52MzsJ+Afga8CbwHNmtsvdXy212XDmBv7k5j8r7K2cxEI21XLM\nghkYaVdvUt4ZbLY4On7dxw83fctoskf/LqXWrrgn0Nf3/v6+4r5Z3sZvBV5z99fd/TfAw8BVMxxP\nRHo0S7JvBn657vs3220isoB6v0FnZtvMbNXMVo8cOdJ3dyJSMEuyHwDOXff9Oe22E7j7dndfcfeV\n5eXlGboTkVnMkuzPARea2flmdgpwHbCrm7BEpGvhu/HuftTMbgb+nab0tsPdX5mgZel4xRZWalO7\nZV27k1m70+2VnaVd1TbR276xZp9UXVfKPHjE6t342K5yLB2/Bmaqs7v7U8BTHcUiIj3SX9CJJKFk\nF0lCyS6ShJJdJAklu0gSM92NjyiVLtyPVRoVSlvhslawVFbaVRkJUz1cL4NdivXBSiB9xDGcSPjh\nAS3B81jtLVQeHP3/XPuxdGUXSULJLpKEkl0kCSW7SBJKdpEkBr8bX771GBi4Erz7WRpYMzaMwECY\n6h336o8cvVUfmKKp0ma4KKKNooeM7KnvjIbf7UCYciNd2UWSULKLJKFkF0lCyS6ShJJdJAklu0gS\nw5be3Cu1rVo5bPS+PspC1UpZZEBOeCq8YM0u0ltktZIe9NFX1/O7xctrw/VVoiu7SBJKdpEklOwi\nSSjZRZJQsoskoWQXSWKm0puZ7Qc+AD4Ejrr7Su35Tm0OuulHXtVLEwMWjaKTyXVdXYvqo6/Yf1pR\n1yH2U+Ybst30rbqos/+hu/+qg+OISI/0Nl4kiVmT3YEfmdnzZrati4BEpB+zvo2/zN0PmNlvA0+b\n2X+6+zPrn9D+EtgGcMYZZ8zYnYhEzXRld/cD7dfDwBPA1hHP2e7uK+6+srS8NEt3IjKDcLKb2bKZ\nnX78MfB1YG9XgYlIt2Z5G78ReMKa0VmfBv7Z3f9tfLPpJ5ysL4MzXTcQr3iVJqr0yhHrI9sqOxdF\neIjgcHGEugqe+2HLa92+QMLJ7u6vA1/sMBYR6ZFKbyJJKNlFklCyiyShZBdJQskuksTga725H5tq\ne/1g5V3V9dym7ykeSA/NOrco5bUeFEOMxl6ZCLT7slxo4cFiE13ZRZJQsoskoWQXSULJLpKEkl0k\niYHvxpeXf4rMQRdftqjSV9cDLgbW+diaIafy6+OggRNSG9hUe81Vm00fRniAVYmu7CJJKNlFklCy\niyShZBdJQskukoSSXSSJwQfCFEsXkTnoggNhamqVlVKH1bEiwbnwokrdhfuqNuz6J+ih+FaY6K8+\n/19sZNCw89NNfzBd2UWSULKLJKFkF0lCyS6ShJJdJAklu0gSY0tvZrYD+AZw2N2/0G47C3gEOA/Y\nD1zr7u9O0mF5KafacLPp28RLXpFhb7GhctGp32L6WO9o4Lri9FGM3TtarbwWLOmGTkitHD398Sa5\nsn8fuOIj224Fdrv7hcDu9nsRWWBjk71db/2dj2y+CtjZPt4JXN1xXCLSsehn9o3ufrB9/BbNiq4i\nssBmvkHnzRQzxQ8XZrbNzFbNbHXtyNqs3YlIUDTZD5nZJoD26+HSE919u7uvuPvK0vJSsDsRmVU0\n2XcBN7aPbwSe7CYcEenLJKW3h4DLgbPN7E3gduBO4FEzuwl4A7h2ot6cyoST5eWfypNARmeH7Ha5\nptBkmYPrYzrHwAyL4RPScaGy9tKpzToanVWycszIq6e8ClX5aGOT3d2vL+z66ri2IrI49Bd0Ikko\n2UWSULKLJKFkF0lCyS6SxMdjwsn6LJAjWXAduNA8hIH4+jJsqa/rcljsPFq1rFWIozpLaK23Sl/l\neljsRwvHOJqu7CJJKNlFklCyiyShZBdJQskukoSSXSSJgUtvjlMY3VarTQw64WRFxyW22uCqShWn\n8wFs/ZTrCqMbg3HEBzF2Pvyu0lXsRVcqD3b9EtCVXSQJJbtIEkp2kSSU7CJJKNlFkliYgTD1wS6j\n99UGu9RjCO3CinHEwqgJVxMCscSXT1qMGfYip7/6egvcOR8XR/WlWngB1foym/46rSu7SBJKdpEk\nlOwiSSjZRZJQsoskoWQXSWKS5Z92AN8ADrv7F9ptdwDfBN5un3abuz81WyjTD4SJLrtUr7pMX8iJ\nHi9aXlucYli3db7pi6+twIpMtbJWdGmo+tmYvmRXL7FOf+4nubJ/H7hixPZ73H1L+2/GRBeRvo1N\ndnd/BnhngFhEpEezfGa/2cz2mNkOMzuzs4hEpBfRZL8PuADYAhwE7io90cy2mdmqma2ura0FuxOR\nWYWS3d0PufuH3iyqfj+wtfLc7e6+4u4rS0tL0ThFZEahZDezTeu+vQbY2004ItKXSUpvDwGXA2eb\n2ZvA7cDlZraFpmqwH/jWxD0Gln8KLRlVCSG6NFS5UbCeVD9oZV+gMNdHiF2LVcNCP1u19FaLo1qW\n67ZgGhmBWWs1Ntnd/foRmx8Y105EFov+gk4kCSW7SBJKdpEklOwiSSjZRZIYfsLJ4rJA3ZbewmW5\nrmtUPUyKWV32KnLAcIiB8mAPy1pFymi12OuTQ1ZGr1WHMU4/HrHWJJISurKLJKFkF0lCyS6ShJJd\nJAklu0gSSnaRJOZQeiuolcqKdYZjlePF+goJj76rHDJYhypVa+o/ch/j3gKj7wLlqXEHLf7ctRJa\nraeOy2tVXlnrLfB/piu7SBJKdpEklOwiSSjZRZJQsoskMfDdeA/dCS/fjY8NhIkPkilsjw5aCd4E\nr48l+RjPQRe80x0anxSc46+Pc1X+0brtTVd2kSSU7CJJKNlFklCyiyShZBdJQskuksQkyz+dC/wA\n2EhTC9ju7vea2VnAI8B5NEtAXevu70YDqQ4wKM1b10PpLSI8yKRWaYodsbx3QeprlbEdYxp23F/X\nxxtzzPp8cqN31k9VPwNhjgLfcfeLgEuBb5vZRcCtwG53vxDY3X4vIgtqbLK7+0F3f6F9/AGwD9gM\nXAXsbJ+2E7i6ryBFZHZTfWY3s/OAi4FngY3ufrDd9RbN23wRWVATJ7uZnQY8Btzi7u+v3+fNh+eR\nHyLMbJuZrZrZ6tqRX88UrIjETZTsZnYyTaI/6O6Pt5sPmdmmdv8m4PCotu6+3d1X3H1lafnULmIW\nkYCxyW5mRrMe+z53v3vdrl3Aje3jG4Enuw9PRLoyyai3LwE3AC+b2YvtttuAO4FHzewm4A3g2n5C\njAlU8ibZ2XEgwSgCJbv68lqVvjqeVq3eV/drQ5VPf23JqO7PVX1AX+Rnm/4/Zmyyu/tPK0f+6tQ9\nishc6C/oRJJQsoskoWQXSULJLpKEkl0kicVZ/qk6MWNh1Fv0eOEyzuh2XVen2s5izabeET1gULW6\nNtzSStEJJ6NiRwzXj0fSlV0kCSW7SBJKdpEklOwiSSjZRZJQsosksUClt3JxolR16XjeyONH7bjF\ngsz02IfawLHA4eoj/YKzc0YiCZcAhy3nTUtXdpEklOwiSSjZRZJQsoskoWQXSWJh7sZXl8epzKxW\nbDPwMkNlCxLIwDeDF+Y0DnW8cQet9VfcV6lQBbrRlV0kCSW7SBJKdpEklOwiSSjZRZJQsoskMbb0\nZmbnAj+gWZLZge3ufq+Z3QF8E3i7fept7v7U2B4jJY9Cm/qYhPLOcFkotExPRR9LIRV2LcpwnPhU\nch2PugkfrzZgq9t9HU+7N1Gd/SjwHXd/wcxOB543s6fbffe4+991G5KI9GGStd4OAgfbxx+Y2T5g\nc9+BiUi3pvrMbmbnARcDz7abbjazPWa2w8zO7Dg2EenQxMluZqcBjwG3uPv7wH3ABcAWmiv/XYV2\n28xs1cxW19Z+3UHIIhIxUbKb2ck0if6guz8O4O6H3P1Ddz8G3A9sHdXW3be7+4q7rywtndpV3CIy\npbHJbs2twgeAfe5+97rtm9Y97Rpgb/fhiUhXJrkb/yXgBuBlM3ux3XYbcL2ZbaGp6uwHvjVbKLUR\nPtPX3rxSJqsXtYYcHhYsiNWG9BV3xc5HXaBlD6e3VtYKHjDYrnbISFmuesCpm0xyN/6nhUOMr6mL\nyMLQX9CJJKFkF0lCyS6ShJJdJAklu0gSH48JJyMT8vVQPimKDimr/tCVyTQDwRTLlzOZ/pjhKlmt\ndFVtF2oViyO6rxBL1xVFXdlFklCyiyShZBdJQskukoSSXSQJJbtIEoOX3iIFlEgZzT5V/j3mlbKW\nVSdznH6kUVWtvFYr1VTLct3Wazov2AXrSd2XUqNxhDobU5YLtKmFUaAru0gSSnaRJJTsIkko2UWS\nULKLJKFkF0li4NKbUSoaREoa9aXeYqWr0BC28EJqlRJaD8ccVmREXB8jFTsuRUb7CpTexkQydQtd\n2UWSULKLJKFkF0lCyS6ShJJdJImxd+PN7LPAM8Bn2uf/0N1vN7PzgYeB3wKeB25w99+MP16xn1oM\nI7fXB7TU1Aa7VBt2bFHiGFD4hntkyaseAonquGIQuYM/yZX9f4GvuPsXaZZnvsLMLgW+C9zj7r8L\nvAvcNH33IjKUscnujf9pvz25/efAV4Afttt3Alf3EqGIdGLS9dlPaldwPQw8DfwCeM/dj7ZPeRPY\n3E+IItKFiZLd3T909y3AOcBW4Pcm7cDMtpnZqpmtrq2tBcMUkVlNdTfe3d8DfgL8AbDBzI7f4DsH\nOFBos93dV9x9ZWlpaaZgRSRubLKb2efMbEP7+FTga8A+mqT/4/ZpNwJP9hWkiMxukoEwm4CdZnYS\nzS+HR939X83sVeBhM/sb4D+ABybrsjQQptuBEwMXVnqQr/Y24HiWfs5u8KCxZqUTUj5RY5Pd3fcA\nF4/Y/jrN53cR+RjQX9CJJKFkF0lCyS6ShJJdJAklu0gSVhs51nlnZm8Db7Tfng38arDOyxTHiRTH\niT5ucfyOu39u1I5Bk/2Ejs1W3X1lLp0rDsWRMA69jRdJQskuksQ8k337HPteT3GcSHGc6BMTx9w+\ns4vIsPQ2XiSJuSS7mV1hZv9lZq+Z2a3ziKGNY7+ZvWxmL5rZ6oD97jCzw2a2d922s8zsaTP7efv1\nzDnFcYeZHWjPyYtmduUAcZxrZj8xs1fN7BUz+/N2+6DnpBLHoOfEzD5rZj8zs5faOP663X6+mT3b\n5s0jZnbKVAd290H/ASfRTGv1eeAU4CXgoqHjaGPZD5w9h36/DFwC7F237W+BW9vHtwLfnVMcdwB/\nMfD52ARc0j4+Hfhv4KKhz0kljkHPCc041dPaxycDzwKXAo8C17Xbvwf86TTHnceVfSvwmru/7s3U\n0w8DV80hjrlx92eAdz6y+SqaiTthoAk8C3EMzt0PuvsL7eMPaCZH2czA56QSx6C80fkkr/NI9s3A\nL9d9P8/JKh34kZk9b2bb5hTDcRvd/WD7+C1g4xxjudnM9rRv83v/OLGemZ1HM3/Cs8zxnHwkDhj4\nnPQxyWv2G3SXufslwB8B3zazL887IGh+szO/qWruAy6gWSPgIHDXUB2b2WnAY8At7v7++n1DnpMR\ncQx+TnyGSV5L5pHsB4Bz131fnKyyb+5+oP16GHiC+c68c8jMNgG0Xw/PIwh3P9S+0I4B9zPQOTGz\nk2kS7EF3f7zdPPg5GRXHvM5J2/fUk7yWzCPZnwMubO8sngJcB+waOggzWzaz048/Br4O7K236tUu\nmok7YY4TeB5PrtY1DHBOrJlM8AFgn7vfvW7XoOekFMfQ56S3SV6HusP4kbuNV9Lc6fwF8JdziuHz\nNJWAl4BXhowDeIjm7eD/0Xz2uolmzbzdwM+BHwNnzSmOfwJeBvbQJNumAeK4jOYt+h7gxfbflUOf\nk0ocg54T4PdpJnHdQ/OL5a/WvWZ/BrwG/AvwmWmOq7+gE0ki+w06kTSU7CJJKNlFklCyiyShZBdJ\nQskukoSSXSQJJbtIEv8PZLN/BOO67fsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(49000, 3073) (1000, 3073) (1000, 3073) (500, 3073)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe-r7FLfGD1h",
        "colab_type": "text"
      },
      "source": [
        "## SVM Classifier\n",
        "\n",
        "Your code for this section will all be written inside **cs231n/classifiers/linear_svm.py**. \n",
        "\n",
        "As you can see, we have prefilled the function `compute_loss_naive` which uses for loops to evaluate the multiclass SVM loss function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXsyA2W1GD1j",
        "colab_type": "code",
        "outputId": "2447c7e7-ff1d-4d22-e5b7-792c818dfedd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Evaluate the naive implementation of the loss we provided for you:\n",
        "from cs231n.classifiers.linear_svm import svm_loss_naive\n",
        "import time\n",
        "\n",
        "# generate a random SVM weight matrix of small numbers\n",
        "W = np.random.randn(3073, 10) * 0.0001 \n",
        "loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "print('loss: %f' % (loss, ))  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 9.544818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml7F2vK8GD1p",
        "colab_type": "text"
      },
      "source": [
        "The `grad` returned from the function above is right now all zero. Derive and implement the gradient for the SVM cost function and implement it inline inside the function `svm_loss_naive`. You will find it helpful to interleave your new code inside the existing function.\n",
        "\n",
        "To check that you have correctly implemented the gradient correctly, you can numerically estimate the gradient of the loss function and compare the numeric estimate to the gradient that you computed. We have provided code that does this for you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z4k7gvEGD1r",
        "colab_type": "code",
        "outputId": "2f3b1b96-885b-4d85-f35c-609958069d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Once you've implemented the gradient, recompute it with the code below\n",
        "# and gradient check it with the function we provided for you\n",
        "\n",
        "# Compute the loss and its gradient at W.\n",
        "loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
        "# compare them with your analytically computed gradient. The numbers should match\n",
        "# almost exactly along all dimensions.\n",
        "from cs231n.gradient_check import grad_check_sparse\n",
        "f = lambda w: svm_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad)\n",
        "print(\"REGU ON\")\n",
        "# do the gradient check once again with regularization turned on\n",
        "# you didn't forget the regularization gradient did you?\n",
        "loss, grad = svm_loss_naive(W, X_dev, y_dev, 5e1)\n",
        "f = lambda w: svm_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numerical: -14.037549 analytic: -14.037549, relative error: 8.549135e-12\n",
            "numerical: 14.812234 analytic: 14.812234, relative error: 1.414803e-11\n",
            "numerical: -25.035010 analytic: -25.035010, relative error: 1.426399e-11\n",
            "numerical: -6.535389 analytic: -6.535389, relative error: 3.238770e-12\n",
            "numerical: 5.466527 analytic: 5.466527, relative error: 4.313059e-11\n",
            "numerical: 23.519922 analytic: 23.519922, relative error: 8.460456e-12\n",
            "numerical: 15.998510 analytic: 15.998510, relative error: 1.156599e-11\n",
            "numerical: -5.578978 analytic: -5.571117, relative error: 7.049941e-04\n",
            "numerical: 13.448918 analytic: 13.448918, relative error: 6.919632e-12\n",
            "numerical: 3.869170 analytic: 3.869170, relative error: 2.705508e-12\n",
            "REGU ON\n",
            "numerical: -13.096918 analytic: -13.109331, relative error: 4.736493e-04\n",
            "numerical: 12.529798 analytic: 12.537831, relative error: 3.204278e-04\n",
            "numerical: -9.007284 analytic: -9.001261, relative error: 3.344485e-04\n",
            "numerical: -1.773190 analytic: -1.777798, relative error: 1.297919e-03\n",
            "numerical: -0.535772 analytic: -0.534639, relative error: 1.057879e-03\n",
            "numerical: 14.004340 analytic: 13.997090, relative error: 2.589488e-04\n",
            "numerical: 5.858764 analytic: 5.852362, relative error: 5.466048e-04\n",
            "numerical: -10.293689 analytic: -10.273939, relative error: 9.602529e-04\n",
            "numerical: 13.071048 analytic: 13.059355, relative error: 4.474896e-04\n",
            "numerical: 6.885419 analytic: 6.876545, relative error: 6.448104e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-inline"
        ],
        "id": "YArEe1vhGD1v",
        "colab_type": "text"
      },
      "source": [
        "**Inline Question 1**\n",
        "\n",
        "It is possible that once in a while a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a gradient check could fail? How would change the margin affect of the frequency of this happening? *Hint: the SVM loss function is not strictly speaking differentiable*\n",
        "\n",
        "$\\color{blue}{\\textit Your Answer:}$ *fill this in.*  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnqIVHgsGD1w",
        "colab_type": "code",
        "outputId": "4569813d-c149-430d-8548-57c1b12cd67f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Next implement the function svm_loss_vectorized; for now only compute the loss;\n",
        "# we will implement the gradient in a moment.\n",
        "tic = time.time()\n",
        "loss_naive, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('Naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
        "\n",
        "from cs231n.classifiers.linear_svm import svm_loss_vectorized\n",
        "tic = time.time()\n",
        "loss_vectorized,_ = svm_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
        "\n",
        "# The losses should match but your vectorized implementation should be much faster.\n",
        "print('difference: %f' % (loss_naive - loss_vectorized))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive loss: 9.544818e+00 computed in 0.140430s\n",
            "Vectorized loss: 9.544818e+00 computed in 0.010791s\n",
            "difference: -0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjLfaEMvGD11",
        "colab_type": "code",
        "outputId": "2f775ffb-56fc-4275-9310-d4c155b61e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Complete the implementation of svm_loss_vectorized, and compute the gradient\n",
        "# of the loss function in a vectorized way.\n",
        "\n",
        "# The naive implementation and the vectorized implementation should match, but\n",
        "# the vectorized version should still be much faster.\n",
        "tic = time.time()\n",
        "_, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
        "\n",
        "tic = time.time()\n",
        "_, grad_vectorized,binary = svm_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
        "\n",
        "# The loss is a single number, so it is easy to compare the values computed\n",
        "# by the two implementations. The gradient on the other hand is a matrix, so\n",
        "# we use the Frobenius norm to compare them.\n",
        "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
        "print('difference: %f' % difference)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive loss and gradient: computed in 0.170004s\n",
            "Vectorized loss and gradient: computed in 0.009224s\n",
            "difference: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtTaK1Z8GD16",
        "colab_type": "text"
      },
      "source": [
        "### Stochastic Gradient Descent\n",
        "\n",
        "We now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8YnTzQJGD17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "2b67514f-27af-4ffd-9264-49d5c0513e52"
      },
      "source": [
        "# In the file linear_classifier.py, implement SGD in the function\n",
        "# LinearClassifier.train() and then run it with the code below.\n",
        "from cs231n.classifiers import LinearSVM\n",
        "svm = LinearSVM()\n",
        "tic = time.time()\n",
        "loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n",
        "                      num_iters=2500, verbose=True)\n",
        "toc = time.time()\n",
        "print('That took %fs' % (toc - tic))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 / 2500: loss 789.847363\n",
            "iteration 100 / 2500: loss 776.815710\n",
            "iteration 200 / 2500: loss 774.320371\n",
            "iteration 300 / 2500: loss 769.845803\n",
            "iteration 400 / 2500: loss 767.415291\n",
            "iteration 500 / 2500: loss 765.411213\n",
            "iteration 600 / 2500: loss 762.636047\n",
            "iteration 700 / 2500: loss 760.999065\n",
            "iteration 800 / 2500: loss 759.036077\n",
            "iteration 900 / 2500: loss 756.043404\n",
            "iteration 1000 / 2500: loss 755.082896\n",
            "iteration 1100 / 2500: loss 752.481444\n",
            "iteration 1200 / 2500: loss 751.505657\n",
            "iteration 1300 / 2500: loss 750.314818\n",
            "iteration 1400 / 2500: loss 748.546898\n",
            "iteration 1500 / 2500: loss 745.859215\n",
            "iteration 1600 / 2500: loss 745.169144\n",
            "iteration 1700 / 2500: loss 743.995267\n",
            "iteration 1800 / 2500: loss 741.591546\n",
            "iteration 1900 / 2500: loss 740.289208\n",
            "iteration 2000 / 2500: loss 739.878973\n",
            "iteration 2100 / 2500: loss 738.744830\n",
            "iteration 2200 / 2500: loss 736.657067\n",
            "iteration 2300 / 2500: loss 735.794186\n",
            "iteration 2400 / 2500: loss 734.354129\n",
            "That took 11.255423s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0Zfn4ErGD2A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "5979f1cc-cf04-463d-d901-55330eb93594"
      },
      "source": [
        "# A useful debugging strategy is to plot the loss as a function of\n",
        "# iteration number:\n",
        "plt.plot(loss_hist)\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAHgCAYAAAAL2HHvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMQUyERERkYgpkImIiIhE7P8BX7cGRD9csGsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVfMWn3TGD2E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7f62b630-5220-4922-f7a6-6ee69b1093c9"
      },
      "source": [
        "# Write the LinearSVM.predict function and evaluate the performance on both the\n",
        "# training and validation set\n",
        "y_train_pred = svm.predict(X_train)\n",
        "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
        "y_val_pred = svm.predict(X_val)\n",
        "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training accuracy: 0.316551\n",
            "validation accuracy: 0.318000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "code"
        ],
        "id": "hmzj3diLGD2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b8e884e-f8a5-47dd-f4f1-b3c237161759"
      },
      "source": [
        "# Use the validation set to tune hyperparameters (regularization strength and\n",
        "# learning rate). You should experiment with different ranges for the learning\n",
        "# rates and regularization strengths; if you are careful you should be able to\n",
        "# get a classification accuracy of about 0.39 on the validation set.\n",
        "\n",
        "#Note: you may see runtime/overflow warnings during hyper-parameter search. \n",
        "# This may be caused by extreme values, and is not a bug.\n",
        "\n",
        "#learning_rates = [1e-7, 5e-5]\n",
        "#regularization_strengths = [2.5e4, 5e4]\n",
        "\n",
        "learning_rates = np.logspace(-7.1, -5.7, num=10)\n",
        "regularization_strengths = np.logspace(2, 5, num=10) #https://gitlab.com/me-learnz/CS231n/blob/master/assignment1/svm.ipynb\n",
        "\n",
        "# results is dictionary mapping tuples of the form\n",
        "# (learning_rate, regularization_strength) to tuples of the form\n",
        "# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
        "# of data points that are correctly classified.\n",
        "results = {}\n",
        "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
        "best_svm = None # The LinearSVM object that achieved the highest validation rate.\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
        "# set. For each combination of hyperparameters, train a linear SVM on the      #\n",
        "# training set, compute its accuracy on the training and validation sets, and  #\n",
        "# store these numbers in the results dictionary. In addition, store the best   #\n",
        "# validation accuracy in best_val and the LinearSVM object that achieves this  #\n",
        "# accuracy in best_svm.                                                        #\n",
        "#                                                                              #\n",
        "# Hint: You should use a small value for num_iters as you develop your         #\n",
        "# validation code so that the SVMs don't take much time to train; once you are #\n",
        "# confident that your validation code works, you should rerun the validation   #\n",
        "# code with a larger value for num_iters.                                      #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "gridsearch=0\n",
        "for i in learning_rates:\n",
        "    for j in regularization_strengths:\n",
        "        gridsearch+=1\n",
        "        print(f\"Grid_search:{gridsearch}\")\n",
        "        svm=LinearSVM()\n",
        "        loss_history=svm.train(X_train,y_train,i,j,num_iters=1500,verbose=True)\n",
        "        y_predict_train=svm.predict(X_train)\n",
        "        y_predict_validate=svm.predict(X_val)\n",
        "        train_accuracy=np.mean(y_predict_train==y_train)\n",
        "        validate_accuracy=np.mean(y_predict_validate==y_val)\n",
        "        results[(i,j)]=(train_accuracy,validate_accuracy)\n",
        "        if validate_accuracy>best_val:\n",
        "            best_val=validate_accuracy\n",
        "            best_svm=svm\n",
        "        del svm\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    \n",
        "# Print out results.\n",
        "\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Grid_search:1\n",
            "iteration 0 / 1500: loss 25.915221\n",
            "iteration 100 / 1500: loss 14.804802\n",
            "iteration 200 / 1500: loss 11.911024\n",
            "iteration 300 / 1500: loss 11.543193\n",
            "iteration 400 / 1500: loss 10.096850\n",
            "iteration 500 / 1500: loss 11.107495\n",
            "iteration 600 / 1500: loss 10.622216\n",
            "iteration 700 / 1500: loss 11.741204\n",
            "iteration 800 / 1500: loss 10.343953\n",
            "iteration 900 / 1500: loss 11.085003\n",
            "iteration 1000 / 1500: loss 10.490655\n",
            "iteration 1100 / 1500: loss 9.951328\n",
            "iteration 1200 / 1500: loss 9.465141\n",
            "iteration 1300 / 1500: loss 9.504528\n",
            "iteration 1400 / 1500: loss 9.170123\n",
            "Grid_search:2\n",
            "iteration 0 / 1500: loss 32.558967\n",
            "iteration 100 / 1500: loss 17.994356\n",
            "iteration 200 / 1500: loss 14.560025\n",
            "iteration 300 / 1500: loss 15.952915\n",
            "iteration 400 / 1500: loss 15.170934\n",
            "iteration 500 / 1500: loss 14.787519\n",
            "iteration 600 / 1500: loss 13.916466\n",
            "iteration 700 / 1500: loss 15.641324\n",
            "iteration 800 / 1500: loss 15.081264\n",
            "iteration 900 / 1500: loss 14.067456\n",
            "iteration 1000 / 1500: loss 14.396886\n",
            "iteration 1100 / 1500: loss 14.566114\n",
            "iteration 1200 / 1500: loss 13.637484\n",
            "iteration 1300 / 1500: loss 13.336508\n",
            "iteration 1400 / 1500: loss 13.606374\n",
            "Grid_search:3\n",
            "iteration 0 / 1500: loss 33.718638\n",
            "iteration 100 / 1500: loss 26.207460\n",
            "iteration 200 / 1500: loss 22.594584\n",
            "iteration 300 / 1500: loss 24.438372\n",
            "iteration 400 / 1500: loss 23.216606\n",
            "iteration 500 / 1500: loss 21.975096\n",
            "iteration 600 / 1500: loss 21.542606\n",
            "iteration 700 / 1500: loss 20.939234\n",
            "iteration 800 / 1500: loss 22.663403\n",
            "iteration 900 / 1500: loss 21.904853\n",
            "iteration 1000 / 1500: loss 20.629506\n",
            "iteration 1100 / 1500: loss 20.417765\n",
            "iteration 1200 / 1500: loss 21.662587\n",
            "iteration 1300 / 1500: loss 21.720477\n",
            "iteration 1400 / 1500: loss 20.216758\n",
            "Grid_search:4\n",
            "iteration 0 / 1500: loss 52.719759\n",
            "iteration 100 / 1500: loss 44.172448\n",
            "iteration 200 / 1500: loss 40.446693\n",
            "iteration 300 / 1500: loss 39.288405\n",
            "iteration 400 / 1500: loss 39.784650\n",
            "iteration 500 / 1500: loss 39.020832\n",
            "iteration 600 / 1500: loss 39.200457\n",
            "iteration 700 / 1500: loss 37.434321\n",
            "iteration 800 / 1500: loss 38.109267\n",
            "iteration 900 / 1500: loss 37.750939\n",
            "iteration 1000 / 1500: loss 36.878383\n",
            "iteration 1100 / 1500: loss 36.607251\n",
            "iteration 1200 / 1500: loss 37.116162\n",
            "iteration 1300 / 1500: loss 36.981219\n",
            "iteration 1400 / 1500: loss 37.510890\n",
            "Grid_search:5\n",
            "iteration 0 / 1500: loss 85.304132\n",
            "iteration 100 / 1500: loss 77.698812\n",
            "iteration 200 / 1500: loss 74.686189\n",
            "iteration 300 / 1500: loss 75.044345\n",
            "iteration 400 / 1500: loss 73.802599\n",
            "iteration 500 / 1500: loss 73.265279\n",
            "iteration 600 / 1500: loss 73.944616\n",
            "iteration 700 / 1500: loss 72.414050\n",
            "iteration 800 / 1500: loss 73.314190\n",
            "iteration 900 / 1500: loss 71.761143\n",
            "iteration 1000 / 1500: loss 72.531517\n",
            "iteration 1100 / 1500: loss 71.367745\n",
            "iteration 1200 / 1500: loss 72.296523\n",
            "iteration 1300 / 1500: loss 71.889852\n",
            "iteration 1400 / 1500: loss 70.759809\n",
            "Grid_search:6\n",
            "iteration 0 / 1500: loss 169.258787\n",
            "iteration 100 / 1500: loss 154.343662\n",
            "iteration 200 / 1500: loss 150.167816\n",
            "iteration 300 / 1500: loss 148.490762\n",
            "iteration 400 / 1500: loss 148.422540\n",
            "iteration 500 / 1500: loss 148.150766\n",
            "iteration 600 / 1500: loss 146.819616\n",
            "iteration 700 / 1500: loss 146.340121\n",
            "iteration 800 / 1500: loss 146.304194\n",
            "iteration 900 / 1500: loss 145.530643\n",
            "iteration 1000 / 1500: loss 145.687584\n",
            "iteration 1100 / 1500: loss 144.904109\n",
            "iteration 1200 / 1500: loss 144.654478\n",
            "iteration 1300 / 1500: loss 144.813328\n",
            "iteration 1400 / 1500: loss 144.683009\n",
            "Grid_search:7\n",
            "iteration 0 / 1500: loss 331.058059\n",
            "iteration 100 / 1500: loss 321.294444\n",
            "iteration 200 / 1500: loss 319.685534\n",
            "iteration 300 / 1500: loss 316.223389\n",
            "iteration 400 / 1500: loss 313.900664\n",
            "iteration 500 / 1500: loss 314.129208\n",
            "iteration 600 / 1500: loss 313.567989\n",
            "iteration 700 / 1500: loss 312.114084\n",
            "iteration 800 / 1500: loss 312.110723\n",
            "iteration 900 / 1500: loss 309.947862\n",
            "iteration 1000 / 1500: loss 310.101665\n",
            "iteration 1100 / 1500: loss 308.980413\n",
            "iteration 1200 / 1500: loss 308.973300\n",
            "iteration 1300 / 1500: loss 308.428416\n",
            "iteration 1400 / 1500: loss 307.138232\n",
            "Grid_search:8\n",
            "iteration 0 / 1500: loss 686.072596\n",
            "iteration 100 / 1500: loss 672.810392\n",
            "iteration 200 / 1500: loss 670.710947\n",
            "iteration 300 / 1500: loss 668.311456\n",
            "iteration 400 / 1500: loss 666.246783\n",
            "iteration 500 / 1500: loss 662.464395\n",
            "iteration 600 / 1500: loss 661.492038\n",
            "iteration 700 / 1500: loss 659.575012\n",
            "iteration 800 / 1500: loss 658.363726\n",
            "iteration 900 / 1500: loss 656.902003\n",
            "iteration 1000 / 1500: loss 656.921572\n",
            "iteration 1100 / 1500: loss 653.316037\n",
            "iteration 1200 / 1500: loss 652.374263\n",
            "iteration 1300 / 1500: loss 650.425100\n",
            "iteration 1400 / 1500: loss 651.834534\n",
            "Grid_search:9\n",
            "iteration 0 / 1500: loss 1447.934784\n",
            "iteration 100 / 1500: loss 1431.745134\n",
            "iteration 200 / 1500: loss 1426.729018\n",
            "iteration 300 / 1500: loss 1420.029155\n",
            "iteration 400 / 1500: loss 1416.857957\n",
            "iteration 500 / 1500: loss 1411.070235\n",
            "iteration 600 / 1500: loss 1408.679656\n",
            "iteration 700 / 1500: loss 1406.277196\n",
            "iteration 800 / 1500: loss 1402.553527\n",
            "iteration 900 / 1500: loss 1399.011202\n",
            "iteration 1000 / 1500: loss 1396.246950\n",
            "iteration 1100 / 1500: loss 1393.093066\n",
            "iteration 1200 / 1500: loss 1391.512269\n",
            "iteration 1300 / 1500: loss 1388.823850\n",
            "iteration 1400 / 1500: loss 1386.060579\n",
            "Grid_search:10\n",
            "iteration 0 / 1500: loss 3096.130920\n",
            "iteration 100 / 1500: loss 3066.644029\n",
            "iteration 200 / 1500: loss 3054.056975\n",
            "iteration 300 / 1500: loss 3045.110767\n",
            "iteration 400 / 1500: loss 3035.606861\n",
            "iteration 500 / 1500: loss 3026.552070\n",
            "iteration 600 / 1500: loss 3018.609055\n",
            "iteration 700 / 1500: loss 3012.431223\n",
            "iteration 800 / 1500: loss 3004.304993\n",
            "iteration 900 / 1500: loss 2997.579354\n",
            "iteration 1000 / 1500: loss 2992.108530\n",
            "iteration 1100 / 1500: loss 2987.026854\n",
            "iteration 1200 / 1500: loss 2983.061851\n",
            "iteration 1300 / 1500: loss 2975.157453\n",
            "iteration 1400 / 1500: loss 2970.533940\n",
            "Grid_search:11\n",
            "iteration 0 / 1500: loss 27.680414\n",
            "iteration 100 / 1500: loss 14.410263\n",
            "iteration 200 / 1500: loss 12.661709\n",
            "iteration 300 / 1500: loss 12.731612\n",
            "iteration 400 / 1500: loss 10.462265\n",
            "iteration 500 / 1500: loss 10.957613\n",
            "iteration 600 / 1500: loss 10.606678\n",
            "iteration 700 / 1500: loss 8.783959\n",
            "iteration 800 / 1500: loss 10.734159\n",
            "iteration 900 / 1500: loss 10.512164\n",
            "iteration 1000 / 1500: loss 9.154694\n",
            "iteration 1100 / 1500: loss 9.078015\n",
            "iteration 1200 / 1500: loss 8.976254\n",
            "iteration 1300 / 1500: loss 8.978373\n",
            "iteration 1400 / 1500: loss 8.929268\n",
            "Grid_search:12\n",
            "iteration 0 / 1500: loss 28.433963\n",
            "iteration 100 / 1500: loss 16.943290\n",
            "iteration 200 / 1500: loss 14.448348\n",
            "iteration 300 / 1500: loss 16.116979\n",
            "iteration 400 / 1500: loss 14.429267\n",
            "iteration 500 / 1500: loss 13.896013\n",
            "iteration 600 / 1500: loss 13.702266\n",
            "iteration 700 / 1500: loss 12.272522\n",
            "iteration 800 / 1500: loss 13.497921\n",
            "iteration 900 / 1500: loss 12.445199\n",
            "iteration 1000 / 1500: loss 14.167111\n",
            "iteration 1100 / 1500: loss 13.135459\n",
            "iteration 1200 / 1500: loss 11.709838\n",
            "iteration 1300 / 1500: loss 12.696299\n",
            "iteration 1400 / 1500: loss 12.483613\n",
            "Grid_search:13\n",
            "iteration 0 / 1500: loss 37.362633\n",
            "iteration 100 / 1500: loss 25.786822\n",
            "iteration 200 / 1500: loss 22.544824\n",
            "iteration 300 / 1500: loss 22.727343\n",
            "iteration 400 / 1500: loss 21.958859\n",
            "iteration 500 / 1500: loss 21.517612\n",
            "iteration 600 / 1500: loss 22.075905\n",
            "iteration 700 / 1500: loss 22.230025\n",
            "iteration 800 / 1500: loss 20.333580\n",
            "iteration 900 / 1500: loss 20.731403\n",
            "iteration 1000 / 1500: loss 20.960092\n",
            "iteration 1100 / 1500: loss 20.462823\n",
            "iteration 1200 / 1500: loss 20.358573\n",
            "iteration 1300 / 1500: loss 20.317167\n",
            "iteration 1400 / 1500: loss 20.034372\n",
            "Grid_search:14\n",
            "iteration 0 / 1500: loss 53.001467\n",
            "iteration 100 / 1500: loss 41.797206\n",
            "iteration 200 / 1500: loss 38.500902\n",
            "iteration 300 / 1500: loss 39.304315\n",
            "iteration 400 / 1500: loss 38.366710\n",
            "iteration 500 / 1500: loss 37.518511\n",
            "iteration 600 / 1500: loss 36.943874\n",
            "iteration 700 / 1500: loss 37.704660\n",
            "iteration 800 / 1500: loss 37.516763\n",
            "iteration 900 / 1500: loss 36.626652\n",
            "iteration 1000 / 1500: loss 37.266850\n",
            "iteration 1100 / 1500: loss 36.335646\n",
            "iteration 1200 / 1500: loss 36.735791\n",
            "iteration 1300 / 1500: loss 35.334202\n",
            "iteration 1400 / 1500: loss 36.493159\n",
            "Grid_search:15\n",
            "iteration 0 / 1500: loss 90.554308\n",
            "iteration 100 / 1500: loss 76.818711\n",
            "iteration 200 / 1500: loss 75.063989\n",
            "iteration 300 / 1500: loss 73.927866\n",
            "iteration 400 / 1500: loss 72.911091\n",
            "iteration 500 / 1500: loss 73.990453\n",
            "iteration 600 / 1500: loss 71.890249\n",
            "iteration 700 / 1500: loss 71.891387\n",
            "iteration 800 / 1500: loss 71.365130\n",
            "iteration 900 / 1500: loss 70.626217\n",
            "iteration 1000 / 1500: loss 71.874023\n",
            "iteration 1100 / 1500: loss 70.454960\n",
            "iteration 1200 / 1500: loss 69.795097\n",
            "iteration 1300 / 1500: loss 70.726555\n",
            "iteration 1400 / 1500: loss 69.912720\n",
            "Grid_search:16\n",
            "iteration 0 / 1500: loss 160.928766\n",
            "iteration 100 / 1500: loss 152.062074\n",
            "iteration 200 / 1500: loss 151.242938\n",
            "iteration 300 / 1500: loss 149.497568\n",
            "iteration 400 / 1500: loss 148.157694\n",
            "iteration 500 / 1500: loss 148.072952\n",
            "iteration 600 / 1500: loss 145.744902\n",
            "iteration 700 / 1500: loss 146.031119\n",
            "iteration 800 / 1500: loss 146.012502\n",
            "iteration 900 / 1500: loss 143.787372\n",
            "iteration 1000 / 1500: loss 144.172789\n",
            "iteration 1100 / 1500: loss 143.817202\n",
            "iteration 1200 / 1500: loss 143.126321\n",
            "iteration 1300 / 1500: loss 144.057512\n",
            "iteration 1400 / 1500: loss 143.917192\n",
            "Grid_search:17\n",
            "iteration 0 / 1500: loss 331.832712\n",
            "iteration 100 / 1500: loss 312.632997\n",
            "iteration 200 / 1500: loss 308.530685\n",
            "iteration 300 / 1500: loss 308.770699\n",
            "iteration 400 / 1500: loss 305.956135\n",
            "iteration 500 / 1500: loss 305.037814\n",
            "iteration 600 / 1500: loss 304.612141\n",
            "iteration 700 / 1500: loss 302.204124\n",
            "iteration 800 / 1500: loss 301.391288\n",
            "iteration 900 / 1500: loss 302.302604\n",
            "iteration 1000 / 1500: loss 300.725165\n",
            "iteration 1100 / 1500: loss 299.571685\n",
            "iteration 1200 / 1500: loss 298.948137\n",
            "iteration 1300 / 1500: loss 298.656735\n",
            "iteration 1400 / 1500: loss 296.919089\n",
            "Grid_search:18\n",
            "iteration 0 / 1500: loss 693.810229\n",
            "iteration 100 / 1500: loss 676.827013\n",
            "iteration 200 / 1500: loss 674.169020\n",
            "iteration 300 / 1500: loss 671.222450\n",
            "iteration 400 / 1500: loss 667.414583\n",
            "iteration 500 / 1500: loss 664.676868\n",
            "iteration 600 / 1500: loss 663.000325\n",
            "iteration 700 / 1500: loss 660.740445\n",
            "iteration 800 / 1500: loss 659.772382\n",
            "iteration 900 / 1500: loss 656.699283\n",
            "iteration 1000 / 1500: loss 656.165691\n",
            "iteration 1100 / 1500: loss 653.427154\n",
            "iteration 1200 / 1500: loss 653.291343\n",
            "iteration 1300 / 1500: loss 649.306542\n",
            "iteration 1400 / 1500: loss 648.720562\n",
            "Grid_search:19\n",
            "iteration 0 / 1500: loss 1458.033204\n",
            "iteration 100 / 1500: loss 1438.353409\n",
            "iteration 200 / 1500: loss 1431.447765\n",
            "iteration 300 / 1500: loss 1423.625871\n",
            "iteration 400 / 1500: loss 1419.725821\n",
            "iteration 500 / 1500: loss 1414.596813\n",
            "iteration 600 / 1500: loss 1411.336988\n",
            "iteration 700 / 1500: loss 1406.634693\n",
            "iteration 800 / 1500: loss 1401.944490\n",
            "iteration 900 / 1500: loss 1398.484574\n",
            "iteration 1000 / 1500: loss 1395.074269\n",
            "iteration 1100 / 1500: loss 1391.451260\n",
            "iteration 1200 / 1500: loss 1388.053293\n",
            "iteration 1300 / 1500: loss 1384.182058\n",
            "iteration 1400 / 1500: loss 1381.286180\n",
            "Grid_search:20\n",
            "iteration 0 / 1500: loss 3092.536177\n",
            "iteration 100 / 1500: loss 3057.989430\n",
            "iteration 200 / 1500: loss 3045.628559\n",
            "iteration 300 / 1500: loss 3032.694345\n",
            "iteration 400 / 1500: loss 3022.763205\n",
            "iteration 500 / 1500: loss 3011.606529\n",
            "iteration 600 / 1500: loss 3001.799150\n",
            "iteration 700 / 1500: loss 2992.553050\n",
            "iteration 800 / 1500: loss 2984.151103\n",
            "iteration 900 / 1500: loss 2976.618924\n",
            "iteration 1000 / 1500: loss 2968.863924\n",
            "iteration 1100 / 1500: loss 2961.780839\n",
            "iteration 1200 / 1500: loss 2952.958053\n",
            "iteration 1300 / 1500: loss 2947.778593\n",
            "iteration 1400 / 1500: loss 2940.563712\n",
            "Grid_search:21\n",
            "iteration 0 / 1500: loss 23.378224\n",
            "iteration 100 / 1500: loss 14.618925\n",
            "iteration 200 / 1500: loss 12.254485\n",
            "iteration 300 / 1500: loss 10.084368\n",
            "iteration 400 / 1500: loss 11.511452\n",
            "iteration 500 / 1500: loss 10.346398\n",
            "iteration 600 / 1500: loss 10.481609\n",
            "iteration 700 / 1500: loss 8.049357\n",
            "iteration 800 / 1500: loss 8.393686\n",
            "iteration 900 / 1500: loss 9.180020\n",
            "iteration 1000 / 1500: loss 8.715543\n",
            "iteration 1100 / 1500: loss 8.777509\n",
            "iteration 1200 / 1500: loss 9.691234\n",
            "iteration 1300 / 1500: loss 9.057318\n",
            "iteration 1400 / 1500: loss 8.632941\n",
            "Grid_search:22\n",
            "iteration 0 / 1500: loss 24.985520\n",
            "iteration 100 / 1500: loss 16.387177\n",
            "iteration 200 / 1500: loss 14.786058\n",
            "iteration 300 / 1500: loss 13.075106\n",
            "iteration 400 / 1500: loss 14.788612\n",
            "iteration 500 / 1500: loss 13.942116\n",
            "iteration 600 / 1500: loss 13.618869\n",
            "iteration 700 / 1500: loss 12.915572\n",
            "iteration 800 / 1500: loss 12.259933\n",
            "iteration 900 / 1500: loss 12.913495\n",
            "iteration 1000 / 1500: loss 12.897590\n",
            "iteration 1100 / 1500: loss 12.416402\n",
            "iteration 1200 / 1500: loss 11.614197\n",
            "iteration 1300 / 1500: loss 12.428855\n",
            "iteration 1400 / 1500: loss 11.931186\n",
            "Grid_search:23\n",
            "iteration 0 / 1500: loss 34.607794\n",
            "iteration 100 / 1500: loss 24.631689\n",
            "iteration 200 / 1500: loss 24.348796\n",
            "iteration 300 / 1500: loss 22.225795\n",
            "iteration 400 / 1500: loss 21.118403\n",
            "iteration 500 / 1500: loss 21.656620\n",
            "iteration 600 / 1500: loss 21.040950\n",
            "iteration 700 / 1500: loss 21.132060\n",
            "iteration 800 / 1500: loss 21.295073\n",
            "iteration 900 / 1500: loss 20.177994\n",
            "iteration 1000 / 1500: loss 20.002823\n",
            "iteration 1100 / 1500: loss 18.924289\n",
            "iteration 1200 / 1500: loss 19.859759\n",
            "iteration 1300 / 1500: loss 19.800461\n",
            "iteration 1400 / 1500: loss 19.097047\n",
            "Grid_search:24\n",
            "iteration 0 / 1500: loss 49.793447\n",
            "iteration 100 / 1500: loss 40.660124\n",
            "iteration 200 / 1500: loss 38.227540\n",
            "iteration 300 / 1500: loss 38.039568\n",
            "iteration 400 / 1500: loss 37.987393\n",
            "iteration 500 / 1500: loss 37.363054\n",
            "iteration 600 / 1500: loss 36.034108\n",
            "iteration 700 / 1500: loss 35.885025\n",
            "iteration 800 / 1500: loss 35.590265\n",
            "iteration 900 / 1500: loss 35.358908\n",
            "iteration 1000 / 1500: loss 35.539638\n",
            "iteration 1100 / 1500: loss 35.409354\n",
            "iteration 1200 / 1500: loss 35.180022\n",
            "iteration 1300 / 1500: loss 35.908550\n",
            "iteration 1400 / 1500: loss 35.359631\n",
            "Grid_search:25\n",
            "iteration 0 / 1500: loss 89.897033\n",
            "iteration 100 / 1500: loss 74.422920\n",
            "iteration 200 / 1500: loss 73.000074\n",
            "iteration 300 / 1500: loss 72.340865\n",
            "iteration 400 / 1500: loss 72.058883\n",
            "iteration 500 / 1500: loss 70.251424\n",
            "iteration 600 / 1500: loss 70.874479\n",
            "iteration 700 / 1500: loss 70.328557\n",
            "iteration 800 / 1500: loss 69.709339\n",
            "iteration 900 / 1500: loss 70.217710\n",
            "iteration 1000 / 1500: loss 70.330269\n",
            "iteration 1100 / 1500: loss 67.781862\n",
            "iteration 1200 / 1500: loss 68.831214\n",
            "iteration 1300 / 1500: loss 68.321451\n",
            "iteration 1400 / 1500: loss 67.264067\n",
            "Grid_search:26\n",
            "iteration 0 / 1500: loss 162.511890\n",
            "iteration 100 / 1500: loss 148.817732\n",
            "iteration 200 / 1500: loss 147.249846\n",
            "iteration 300 / 1500: loss 146.565706\n",
            "iteration 400 / 1500: loss 144.668551\n",
            "iteration 500 / 1500: loss 143.811728\n",
            "iteration 600 / 1500: loss 143.360649\n",
            "iteration 700 / 1500: loss 142.729204\n",
            "iteration 800 / 1500: loss 142.501876\n",
            "iteration 900 / 1500: loss 142.249842\n",
            "iteration 1000 / 1500: loss 140.673683\n",
            "iteration 1100 / 1500: loss 140.513018\n",
            "iteration 1200 / 1500: loss 139.100284\n",
            "iteration 1300 / 1500: loss 139.456727\n",
            "iteration 1400 / 1500: loss 138.685211\n",
            "Grid_search:27\n",
            "iteration 0 / 1500: loss 334.360107\n",
            "iteration 100 / 1500: loss 312.456425\n",
            "iteration 200 / 1500: loss 308.444240\n",
            "iteration 300 / 1500: loss 309.404546\n",
            "iteration 400 / 1500: loss 305.204234\n",
            "iteration 500 / 1500: loss 304.614352\n",
            "iteration 600 / 1500: loss 304.219305\n",
            "iteration 700 / 1500: loss 302.029048\n",
            "iteration 800 / 1500: loss 301.347102\n",
            "iteration 900 / 1500: loss 299.216553\n",
            "iteration 1000 / 1500: loss 298.244355\n",
            "iteration 1100 / 1500: loss 297.075878\n",
            "iteration 1200 / 1500: loss 297.271357\n",
            "iteration 1300 / 1500: loss 296.424450\n",
            "iteration 1400 / 1500: loss 294.772622\n",
            "Grid_search:28\n",
            "iteration 0 / 1500: loss 682.838250\n",
            "iteration 100 / 1500: loss 664.132691\n",
            "iteration 200 / 1500: loss 660.322393\n",
            "iteration 300 / 1500: loss 657.357728\n",
            "iteration 400 / 1500: loss 654.259529\n",
            "iteration 500 / 1500: loss 651.005651\n",
            "iteration 600 / 1500: loss 648.280248\n",
            "iteration 700 / 1500: loss 645.839523\n",
            "iteration 800 / 1500: loss 643.498875\n",
            "iteration 900 / 1500: loss 642.219222\n",
            "iteration 1000 / 1500: loss 639.125754\n",
            "iteration 1100 / 1500: loss 637.501453\n",
            "iteration 1200 / 1500: loss 635.457603\n",
            "iteration 1300 / 1500: loss 633.871687\n",
            "iteration 1400 / 1500: loss 632.703599\n",
            "Grid_search:29\n",
            "iteration 0 / 1500: loss 1447.352589\n",
            "iteration 100 / 1500: loss 1419.310989\n",
            "iteration 200 / 1500: loss 1411.032538\n",
            "iteration 300 / 1500: loss 1404.758086\n",
            "iteration 400 / 1500: loss 1397.477772\n",
            "iteration 500 / 1500: loss 1391.409091\n",
            "iteration 600 / 1500: loss 1386.242803\n",
            "iteration 700 / 1500: loss 1380.425366\n",
            "iteration 800 / 1500: loss 1376.203990\n",
            "iteration 900 / 1500: loss 1371.694542\n",
            "iteration 1000 / 1500: loss 1368.075254\n",
            "iteration 1100 / 1500: loss 1363.728127\n",
            "iteration 1200 / 1500: loss 1358.964608\n",
            "iteration 1300 / 1500: loss 1356.704490\n",
            "iteration 1400 / 1500: loss 1353.364449\n",
            "Grid_search:30\n",
            "iteration 0 / 1500: loss 3077.307194\n",
            "iteration 100 / 1500: loss 3041.441091\n",
            "iteration 200 / 1500: loss 3023.908275\n",
            "iteration 300 / 1500: loss 3008.055172\n",
            "iteration 400 / 1500: loss 2995.213022\n",
            "iteration 500 / 1500: loss 2981.761691\n",
            "iteration 600 / 1500: loss 2971.587069\n",
            "iteration 700 / 1500: loss 2960.181555\n",
            "iteration 800 / 1500: loss 2951.514542\n",
            "iteration 900 / 1500: loss 2941.217384\n",
            "iteration 1000 / 1500: loss 2932.081006\n",
            "iteration 1100 / 1500: loss 2923.481268\n",
            "iteration 1200 / 1500: loss 2914.795672\n",
            "iteration 1300 / 1500: loss 2906.244828\n",
            "iteration 1400 / 1500: loss 2899.652800\n",
            "Grid_search:31\n",
            "iteration 0 / 1500: loss 23.918740\n",
            "iteration 100 / 1500: loss 12.640018\n",
            "iteration 200 / 1500: loss 11.729968\n",
            "iteration 300 / 1500: loss 11.537657\n",
            "iteration 400 / 1500: loss 10.833847\n",
            "iteration 500 / 1500: loss 9.500426\n",
            "iteration 600 / 1500: loss 10.209027\n",
            "iteration 700 / 1500: loss 9.885503\n",
            "iteration 800 / 1500: loss 8.476168\n",
            "iteration 900 / 1500: loss 9.337583\n",
            "iteration 1000 / 1500: loss 7.756749\n",
            "iteration 1100 / 1500: loss 8.377706\n",
            "iteration 1200 / 1500: loss 8.581222\n",
            "iteration 1300 / 1500: loss 8.601071\n",
            "iteration 1400 / 1500: loss 8.183306\n",
            "Grid_search:32\n",
            "iteration 0 / 1500: loss 25.147271\n",
            "iteration 100 / 1500: loss 16.969807\n",
            "iteration 200 / 1500: loss 15.663657\n",
            "iteration 300 / 1500: loss 13.745225\n",
            "iteration 400 / 1500: loss 13.161549\n",
            "iteration 500 / 1500: loss 14.064379\n",
            "iteration 600 / 1500: loss 13.099144\n",
            "iteration 700 / 1500: loss 11.700049\n",
            "iteration 800 / 1500: loss 13.104922\n",
            "iteration 900 / 1500: loss 12.081521\n",
            "iteration 1000 / 1500: loss 12.292120\n",
            "iteration 1100 / 1500: loss 12.300576\n",
            "iteration 1200 / 1500: loss 12.638660\n",
            "iteration 1300 / 1500: loss 12.173020\n",
            "iteration 1400 / 1500: loss 11.444925\n",
            "Grid_search:33\n",
            "iteration 0 / 1500: loss 43.760525\n",
            "iteration 100 / 1500: loss 23.690596\n",
            "iteration 200 / 1500: loss 21.343819\n",
            "iteration 300 / 1500: loss 20.864136\n",
            "iteration 400 / 1500: loss 20.985024\n",
            "iteration 500 / 1500: loss 20.661928\n",
            "iteration 600 / 1500: loss 20.109536\n",
            "iteration 700 / 1500: loss 20.866483\n",
            "iteration 800 / 1500: loss 19.994388\n",
            "iteration 900 / 1500: loss 20.166560\n",
            "iteration 1000 / 1500: loss 19.566356\n",
            "iteration 1100 / 1500: loss 19.164827\n",
            "iteration 1200 / 1500: loss 19.557439\n",
            "iteration 1300 / 1500: loss 19.402651\n",
            "iteration 1400 / 1500: loss 19.125169\n",
            "Grid_search:34\n",
            "iteration 0 / 1500: loss 51.921189\n",
            "iteration 100 / 1500: loss 40.019687\n",
            "iteration 200 / 1500: loss 38.086461\n",
            "iteration 300 / 1500: loss 37.484709\n",
            "iteration 400 / 1500: loss 37.373329\n",
            "iteration 500 / 1500: loss 35.958886\n",
            "iteration 600 / 1500: loss 35.505651\n",
            "iteration 700 / 1500: loss 35.573153\n",
            "iteration 800 / 1500: loss 35.904275\n",
            "iteration 900 / 1500: loss 35.038128\n",
            "iteration 1000 / 1500: loss 34.260399\n",
            "iteration 1100 / 1500: loss 35.310585\n",
            "iteration 1200 / 1500: loss 35.081097\n",
            "iteration 1300 / 1500: loss 34.957781\n",
            "iteration 1400 / 1500: loss 35.202974\n",
            "Grid_search:35\n",
            "iteration 0 / 1500: loss 83.889033\n",
            "iteration 100 / 1500: loss 74.746245\n",
            "iteration 200 / 1500: loss 73.927097\n",
            "iteration 300 / 1500: loss 71.654976\n",
            "iteration 400 / 1500: loss 70.513227\n",
            "iteration 500 / 1500: loss 70.963405\n",
            "iteration 600 / 1500: loss 70.704036\n",
            "iteration 700 / 1500: loss 69.041999\n",
            "iteration 800 / 1500: loss 68.872910\n",
            "iteration 900 / 1500: loss 70.406435\n",
            "iteration 1000 / 1500: loss 67.841778\n",
            "iteration 1100 / 1500: loss 68.523206\n",
            "iteration 1200 / 1500: loss 67.747343\n",
            "iteration 1300 / 1500: loss 66.981268\n",
            "iteration 1400 / 1500: loss 68.173683\n",
            "Grid_search:36\n",
            "iteration 0 / 1500: loss 166.767529\n",
            "iteration 100 / 1500: loss 151.441155\n",
            "iteration 200 / 1500: loss 148.224127\n",
            "iteration 300 / 1500: loss 146.486549\n",
            "iteration 400 / 1500: loss 145.193779\n",
            "iteration 500 / 1500: loss 144.647296\n",
            "iteration 600 / 1500: loss 143.413528\n",
            "iteration 700 / 1500: loss 142.920939\n",
            "iteration 800 / 1500: loss 141.118239\n",
            "iteration 900 / 1500: loss 140.816909\n",
            "iteration 1000 / 1500: loss 140.681733\n",
            "iteration 1100 / 1500: loss 139.793918\n",
            "iteration 1200 / 1500: loss 138.813442\n",
            "iteration 1300 / 1500: loss 138.597101\n",
            "iteration 1400 / 1500: loss 137.750795\n",
            "Grid_search:37\n",
            "iteration 0 / 1500: loss 334.559994\n",
            "iteration 100 / 1500: loss 315.879314\n",
            "iteration 200 / 1500: loss 313.476438\n",
            "iteration 300 / 1500: loss 310.173731\n",
            "iteration 400 / 1500: loss 308.550228\n",
            "iteration 500 / 1500: loss 307.652388\n",
            "iteration 600 / 1500: loss 305.058594\n",
            "iteration 700 / 1500: loss 303.220800\n",
            "iteration 800 / 1500: loss 302.111376\n",
            "iteration 900 / 1500: loss 300.568963\n",
            "iteration 1000 / 1500: loss 299.961264\n",
            "iteration 1100 / 1500: loss 299.126326\n",
            "iteration 1200 / 1500: loss 296.786562\n",
            "iteration 1300 / 1500: loss 297.575552\n",
            "iteration 1400 / 1500: loss 294.978913\n",
            "Grid_search:38\n",
            "iteration 0 / 1500: loss 683.063022\n",
            "iteration 100 / 1500: loss 665.357063\n",
            "iteration 200 / 1500: loss 659.215200\n",
            "iteration 300 / 1500: loss 654.551457\n",
            "iteration 400 / 1500: loss 651.413957\n",
            "iteration 500 / 1500: loss 647.976721\n",
            "iteration 600 / 1500: loss 645.996209\n",
            "iteration 700 / 1500: loss 640.663117\n",
            "iteration 800 / 1500: loss 639.406806\n",
            "iteration 900 / 1500: loss 636.375340\n",
            "iteration 1000 / 1500: loss 634.151627\n",
            "iteration 1100 / 1500: loss 631.595141\n",
            "iteration 1200 / 1500: loss 629.909626\n",
            "iteration 1300 / 1500: loss 628.160984\n",
            "iteration 1400 / 1500: loss 625.130488\n",
            "Grid_search:39\n",
            "iteration 0 / 1500: loss 1450.390405\n",
            "iteration 100 / 1500: loss 1417.414900\n",
            "iteration 200 / 1500: loss 1406.632981\n",
            "iteration 300 / 1500: loss 1396.756745\n",
            "iteration 400 / 1500: loss 1389.335184\n",
            "iteration 500 / 1500: loss 1382.838692\n",
            "iteration 600 / 1500: loss 1376.404820\n",
            "iteration 700 / 1500: loss 1369.650275\n",
            "iteration 800 / 1500: loss 1364.984778\n",
            "iteration 900 / 1500: loss 1359.098557\n",
            "iteration 1000 / 1500: loss 1354.297654\n",
            "iteration 1100 / 1500: loss 1348.607022\n",
            "iteration 1200 / 1500: loss 1343.734679\n",
            "iteration 1300 / 1500: loss 1340.296241\n",
            "iteration 1400 / 1500: loss 1335.107286\n",
            "Grid_search:40\n",
            "iteration 0 / 1500: loss 3117.465486\n",
            "iteration 100 / 1500: loss 3068.327146\n",
            "iteration 200 / 1500: loss 3045.267061\n",
            "iteration 300 / 1500: loss 3027.636146\n",
            "iteration 400 / 1500: loss 3009.671512\n",
            "iteration 500 / 1500: loss 2996.322107\n",
            "iteration 600 / 1500: loss 2980.671176\n",
            "iteration 700 / 1500: loss 2968.224029\n",
            "iteration 800 / 1500: loss 2956.794404\n",
            "iteration 900 / 1500: loss 2944.237575\n",
            "iteration 1000 / 1500: loss 2932.677999\n",
            "iteration 1100 / 1500: loss 2922.390764\n",
            "iteration 1200 / 1500: loss 2911.803398\n",
            "iteration 1300 / 1500: loss 2902.579844\n",
            "iteration 1400 / 1500: loss 2892.939941\n",
            "Grid_search:41\n",
            "iteration 0 / 1500: loss 28.037202\n",
            "iteration 100 / 1500: loss 11.993731\n",
            "iteration 200 / 1500: loss 11.108409\n",
            "iteration 300 / 1500: loss 10.800398\n",
            "iteration 400 / 1500: loss 9.683864\n",
            "iteration 500 / 1500: loss 9.332310\n",
            "iteration 600 / 1500: loss 9.489216\n",
            "iteration 700 / 1500: loss 9.181436\n",
            "iteration 800 / 1500: loss 9.253288\n",
            "iteration 900 / 1500: loss 9.179167\n",
            "iteration 1000 / 1500: loss 8.753953\n",
            "iteration 1100 / 1500: loss 8.641254\n",
            "iteration 1200 / 1500: loss 9.043456\n",
            "iteration 1300 / 1500: loss 8.172202\n",
            "iteration 1400 / 1500: loss 8.641981\n",
            "Grid_search:42\n",
            "iteration 0 / 1500: loss 32.609004\n",
            "iteration 100 / 1500: loss 14.281537\n",
            "iteration 200 / 1500: loss 14.021681\n",
            "iteration 300 / 1500: loss 13.742441\n",
            "iteration 400 / 1500: loss 12.262531\n",
            "iteration 500 / 1500: loss 13.506761\n",
            "iteration 600 / 1500: loss 12.222971\n",
            "iteration 700 / 1500: loss 13.885444\n",
            "iteration 800 / 1500: loss 12.120282\n",
            "iteration 900 / 1500: loss 12.085198\n",
            "iteration 1000 / 1500: loss 12.588061\n",
            "iteration 1100 / 1500: loss 11.622890\n",
            "iteration 1200 / 1500: loss 11.861281\n",
            "iteration 1300 / 1500: loss 11.680041\n",
            "iteration 1400 / 1500: loss 11.619400\n",
            "Grid_search:43\n",
            "iteration 0 / 1500: loss 40.900509\n",
            "iteration 100 / 1500: loss 22.599994\n",
            "iteration 200 / 1500: loss 21.080155\n",
            "iteration 300 / 1500: loss 21.021070\n",
            "iteration 400 / 1500: loss 20.196791\n",
            "iteration 500 / 1500: loss 19.767829\n",
            "iteration 600 / 1500: loss 19.739767\n",
            "iteration 700 / 1500: loss 19.040324\n",
            "iteration 800 / 1500: loss 19.795649\n",
            "iteration 900 / 1500: loss 18.894971\n",
            "iteration 1000 / 1500: loss 19.408065\n",
            "iteration 1100 / 1500: loss 18.174828\n",
            "iteration 1200 / 1500: loss 18.406581\n",
            "iteration 1300 / 1500: loss 18.837664\n",
            "iteration 1400 / 1500: loss 18.784212\n",
            "Grid_search:44\n",
            "iteration 0 / 1500: loss 52.095107\n",
            "iteration 100 / 1500: loss 40.857375\n",
            "iteration 200 / 1500: loss 38.792120\n",
            "iteration 300 / 1500: loss 36.438745\n",
            "iteration 400 / 1500: loss 37.439440\n",
            "iteration 500 / 1500: loss 36.246328\n",
            "iteration 600 / 1500: loss 36.176234\n",
            "iteration 700 / 1500: loss 35.192620\n",
            "iteration 800 / 1500: loss 35.830481\n",
            "iteration 900 / 1500: loss 34.964568\n",
            "iteration 1000 / 1500: loss 34.712837\n",
            "iteration 1100 / 1500: loss 35.048451\n",
            "iteration 1200 / 1500: loss 34.069178\n",
            "iteration 1300 / 1500: loss 34.992320\n",
            "iteration 1400 / 1500: loss 33.938730\n",
            "Grid_search:45\n",
            "iteration 0 / 1500: loss 89.658625\n",
            "iteration 100 / 1500: loss 73.600937\n",
            "iteration 200 / 1500: loss 71.874435\n",
            "iteration 300 / 1500: loss 71.342001\n",
            "iteration 400 / 1500: loss 71.283368\n",
            "iteration 500 / 1500: loss 69.880158\n",
            "iteration 600 / 1500: loss 68.858739\n",
            "iteration 700 / 1500: loss 69.222807\n",
            "iteration 800 / 1500: loss 68.984087\n",
            "iteration 900 / 1500: loss 67.447604\n",
            "iteration 1000 / 1500: loss 67.148742\n",
            "iteration 1100 / 1500: loss 68.285807\n",
            "iteration 1200 / 1500: loss 65.965211\n",
            "iteration 1300 / 1500: loss 66.124578\n",
            "iteration 1400 / 1500: loss 65.590349\n",
            "Grid_search:46\n",
            "iteration 0 / 1500: loss 166.804431\n",
            "iteration 100 / 1500: loss 148.746775\n",
            "iteration 200 / 1500: loss 147.134458\n",
            "iteration 300 / 1500: loss 145.151036\n",
            "iteration 400 / 1500: loss 142.387617\n",
            "iteration 500 / 1500: loss 142.006983\n",
            "iteration 600 / 1500: loss 141.174480\n",
            "iteration 700 / 1500: loss 140.137955\n",
            "iteration 800 / 1500: loss 139.583584\n",
            "iteration 900 / 1500: loss 139.455981\n",
            "iteration 1000 / 1500: loss 137.443060\n",
            "iteration 1100 / 1500: loss 136.983129\n",
            "iteration 1200 / 1500: loss 136.615284\n",
            "iteration 1300 / 1500: loss 135.960516\n",
            "iteration 1400 / 1500: loss 135.544044\n",
            "Grid_search:47\n",
            "iteration 0 / 1500: loss 328.833486\n",
            "iteration 100 / 1500: loss 311.018229\n",
            "iteration 200 / 1500: loss 307.479621\n",
            "iteration 300 / 1500: loss 303.974542\n",
            "iteration 400 / 1500: loss 302.301972\n",
            "iteration 500 / 1500: loss 300.753048\n",
            "iteration 600 / 1500: loss 298.150182\n",
            "iteration 700 / 1500: loss 296.591932\n",
            "iteration 800 / 1500: loss 295.843953\n",
            "iteration 900 / 1500: loss 294.559696\n",
            "iteration 1000 / 1500: loss 293.383832\n",
            "iteration 1100 / 1500: loss 291.807858\n",
            "iteration 1200 / 1500: loss 290.286935\n",
            "iteration 1300 / 1500: loss 289.742921\n",
            "iteration 1400 / 1500: loss 288.474502\n",
            "Grid_search:48\n",
            "iteration 0 / 1500: loss 685.547909\n",
            "iteration 100 / 1500: loss 663.741043\n",
            "iteration 200 / 1500: loss 655.960589\n",
            "iteration 300 / 1500: loss 649.294508\n",
            "iteration 400 / 1500: loss 644.229220\n",
            "iteration 500 / 1500: loss 641.669584\n",
            "iteration 600 / 1500: loss 637.572747\n",
            "iteration 700 / 1500: loss 633.191532\n",
            "iteration 800 / 1500: loss 630.458976\n",
            "iteration 900 / 1500: loss 627.916365\n",
            "iteration 1000 / 1500: loss 624.575695\n",
            "iteration 1100 / 1500: loss 622.864625\n",
            "iteration 1200 / 1500: loss 619.631374\n",
            "iteration 1300 / 1500: loss 617.449963\n",
            "iteration 1400 / 1500: loss 614.567595\n",
            "Grid_search:49\n",
            "iteration 0 / 1500: loss 1433.842917\n",
            "iteration 100 / 1500: loss 1400.982384\n",
            "iteration 200 / 1500: loss 1387.522603\n",
            "iteration 300 / 1500: loss 1375.628685\n",
            "iteration 400 / 1500: loss 1364.887285\n",
            "iteration 500 / 1500: loss 1357.037696\n",
            "iteration 600 / 1500: loss 1348.315617\n",
            "iteration 700 / 1500: loss 1340.594272\n",
            "iteration 800 / 1500: loss 1335.556451\n",
            "iteration 900 / 1500: loss 1327.616484\n",
            "iteration 1000 / 1500: loss 1321.580833\n",
            "iteration 1100 / 1500: loss 1316.122159\n",
            "iteration 1200 / 1500: loss 1311.428204\n",
            "iteration 1300 / 1500: loss 1306.050489\n",
            "iteration 1400 / 1500: loss 1301.413663\n",
            "Grid_search:50\n",
            "iteration 0 / 1500: loss 3080.614201\n",
            "iteration 100 / 1500: loss 3025.274164\n",
            "iteration 200 / 1500: loss 2995.192117\n",
            "iteration 300 / 1500: loss 2972.743584\n",
            "iteration 400 / 1500: loss 2952.031329\n",
            "iteration 500 / 1500: loss 2933.933600\n",
            "iteration 600 / 1500: loss 2917.083945\n",
            "iteration 700 / 1500: loss 2900.359733\n",
            "iteration 800 / 1500: loss 2886.221316\n",
            "iteration 900 / 1500: loss 2874.361734\n",
            "iteration 1000 / 1500: loss 2859.798352\n",
            "iteration 1100 / 1500: loss 2848.850926\n",
            "iteration 1200 / 1500: loss 2837.558216\n",
            "iteration 1300 / 1500: loss 2825.804488\n",
            "iteration 1400 / 1500: loss 2815.242372\n",
            "Grid_search:51\n",
            "iteration 0 / 1500: loss 21.796647\n",
            "iteration 100 / 1500: loss 10.632687\n",
            "iteration 200 / 1500: loss 9.624291\n",
            "iteration 300 / 1500: loss 9.057021\n",
            "iteration 400 / 1500: loss 8.674194\n",
            "iteration 500 / 1500: loss 9.854366\n",
            "iteration 600 / 1500: loss 9.586040\n",
            "iteration 700 / 1500: loss 8.624122\n",
            "iteration 800 / 1500: loss 8.842829\n",
            "iteration 900 / 1500: loss 8.684428\n",
            "iteration 1000 / 1500: loss 9.227289\n",
            "iteration 1100 / 1500: loss 9.125710\n",
            "iteration 1200 / 1500: loss 8.415218\n",
            "iteration 1300 / 1500: loss 8.432141\n",
            "iteration 1400 / 1500: loss 7.721404\n",
            "Grid_search:52\n",
            "iteration 0 / 1500: loss 23.513197\n",
            "iteration 100 / 1500: loss 14.293068\n",
            "iteration 200 / 1500: loss 12.413352\n",
            "iteration 300 / 1500: loss 12.903827\n",
            "iteration 400 / 1500: loss 12.250521\n",
            "iteration 500 / 1500: loss 11.994060\n",
            "iteration 600 / 1500: loss 12.849897\n",
            "iteration 700 / 1500: loss 12.114041\n",
            "iteration 800 / 1500: loss 11.972558\n",
            "iteration 900 / 1500: loss 12.162800\n",
            "iteration 1000 / 1500: loss 11.839465\n",
            "iteration 1100 / 1500: loss 10.895397\n",
            "iteration 1200 / 1500: loss 11.348789\n",
            "iteration 1300 / 1500: loss 11.791949\n",
            "iteration 1400 / 1500: loss 11.431785\n",
            "Grid_search:53\n",
            "iteration 0 / 1500: loss 42.290661\n",
            "iteration 100 / 1500: loss 22.515259\n",
            "iteration 200 / 1500: loss 20.756242\n",
            "iteration 300 / 1500: loss 20.263374\n",
            "iteration 400 / 1500: loss 19.146584\n",
            "iteration 500 / 1500: loss 19.009678\n",
            "iteration 600 / 1500: loss 19.284030\n",
            "iteration 700 / 1500: loss 19.224946\n",
            "iteration 800 / 1500: loss 18.729381\n",
            "iteration 900 / 1500: loss 18.827727\n",
            "iteration 1000 / 1500: loss 18.485881\n",
            "iteration 1100 / 1500: loss 18.101838\n",
            "iteration 1200 / 1500: loss 18.268132\n",
            "iteration 1300 / 1500: loss 17.515251\n",
            "iteration 1400 / 1500: loss 17.935013\n",
            "Grid_search:54\n",
            "iteration 0 / 1500: loss 55.990023\n",
            "iteration 100 / 1500: loss 38.308020\n",
            "iteration 200 / 1500: loss 36.544471\n",
            "iteration 300 / 1500: loss 36.712971\n",
            "iteration 400 / 1500: loss 35.438879\n",
            "iteration 500 / 1500: loss 35.153189\n",
            "iteration 600 / 1500: loss 35.928254\n",
            "iteration 700 / 1500: loss 34.476423\n",
            "iteration 800 / 1500: loss 34.131641\n",
            "iteration 900 / 1500: loss 33.770951\n",
            "iteration 1000 / 1500: loss 33.659221\n",
            "iteration 1100 / 1500: loss 33.841866\n",
            "iteration 1200 / 1500: loss 32.182647\n",
            "iteration 1300 / 1500: loss 33.085393\n",
            "iteration 1400 / 1500: loss 32.636543\n",
            "Grid_search:55\n",
            "iteration 0 / 1500: loss 87.740469\n",
            "iteration 100 / 1500: loss 74.014023\n",
            "iteration 200 / 1500: loss 72.648459\n",
            "iteration 300 / 1500: loss 72.041262\n",
            "iteration 400 / 1500: loss 70.347718\n",
            "iteration 500 / 1500: loss 69.754786\n",
            "iteration 600 / 1500: loss 69.033540\n",
            "iteration 700 / 1500: loss 68.890413\n",
            "iteration 800 / 1500: loss 67.691034\n",
            "iteration 900 / 1500: loss 67.290131\n",
            "iteration 1000 / 1500: loss 66.752605\n",
            "iteration 1100 / 1500: loss 66.458746\n",
            "iteration 1200 / 1500: loss 67.107760\n",
            "iteration 1300 / 1500: loss 66.455440\n",
            "iteration 1400 / 1500: loss 66.218968\n",
            "Grid_search:56\n",
            "iteration 0 / 1500: loss 162.794379\n",
            "iteration 100 / 1500: loss 148.511554\n",
            "iteration 200 / 1500: loss 145.586566\n",
            "iteration 300 / 1500: loss 143.371860\n",
            "iteration 400 / 1500: loss 142.986403\n",
            "iteration 500 / 1500: loss 140.754199\n",
            "iteration 600 / 1500: loss 140.630272\n",
            "iteration 700 / 1500: loss 138.927124\n",
            "iteration 800 / 1500: loss 139.450406\n",
            "iteration 900 / 1500: loss 137.056018\n",
            "iteration 1000 / 1500: loss 136.665259\n",
            "iteration 1100 / 1500: loss 136.196394\n",
            "iteration 1200 / 1500: loss 135.225141\n",
            "iteration 1300 / 1500: loss 134.476746\n",
            "iteration 1400 / 1500: loss 133.786217\n",
            "Grid_search:57\n",
            "iteration 0 / 1500: loss 324.559875\n",
            "iteration 100 / 1500: loss 308.650356\n",
            "iteration 200 / 1500: loss 303.669733\n",
            "iteration 300 / 1500: loss 300.595746\n",
            "iteration 400 / 1500: loss 297.048866\n",
            "iteration 500 / 1500: loss 294.932976\n",
            "iteration 600 / 1500: loss 293.922573\n",
            "iteration 700 / 1500: loss 291.525009\n",
            "iteration 800 / 1500: loss 290.156321\n",
            "iteration 900 / 1500: loss 287.478073\n",
            "iteration 1000 / 1500: loss 286.137015\n",
            "iteration 1100 / 1500: loss 285.404064\n",
            "iteration 1200 / 1500: loss 284.183907\n",
            "iteration 1300 / 1500: loss 283.362635\n",
            "iteration 1400 / 1500: loss 281.417974\n",
            "Grid_search:58\n",
            "iteration 0 / 1500: loss 681.162962\n",
            "iteration 100 / 1500: loss 658.553903\n",
            "iteration 200 / 1500: loss 649.352815\n",
            "iteration 300 / 1500: loss 642.989803\n",
            "iteration 400 / 1500: loss 637.653642\n",
            "iteration 500 / 1500: loss 632.641330\n",
            "iteration 600 / 1500: loss 628.182546\n",
            "iteration 700 / 1500: loss 623.093344\n",
            "iteration 800 / 1500: loss 619.862392\n",
            "iteration 900 / 1500: loss 618.046462\n",
            "iteration 1000 / 1500: loss 614.121506\n",
            "iteration 1100 / 1500: loss 610.766324\n",
            "iteration 1200 / 1500: loss 607.974894\n",
            "iteration 1300 / 1500: loss 605.598990\n",
            "iteration 1400 / 1500: loss 601.729834\n",
            "Grid_search:59\n",
            "iteration 0 / 1500: loss 1434.306608\n",
            "iteration 100 / 1500: loss 1397.336008\n",
            "iteration 200 / 1500: loss 1378.524403\n",
            "iteration 300 / 1500: loss 1366.309819\n",
            "iteration 400 / 1500: loss 1352.980626\n",
            "iteration 500 / 1500: loss 1342.337978\n",
            "iteration 600 / 1500: loss 1333.182573\n",
            "iteration 700 / 1500: loss 1325.245202\n",
            "iteration 800 / 1500: loss 1316.610753\n",
            "iteration 900 / 1500: loss 1310.262086\n",
            "iteration 1000 / 1500: loss 1302.933258\n",
            "iteration 1100 / 1500: loss 1297.305453\n",
            "iteration 1200 / 1500: loss 1290.019037\n",
            "iteration 1300 / 1500: loss 1285.725288\n",
            "iteration 1400 / 1500: loss 1280.702265\n",
            "Grid_search:60\n",
            "iteration 0 / 1500: loss 3102.990499\n",
            "iteration 100 / 1500: loss 3031.100473\n",
            "iteration 200 / 1500: loss 2992.530121\n",
            "iteration 300 / 1500: loss 2963.193059\n",
            "iteration 400 / 1500: loss 2937.772290\n",
            "iteration 500 / 1500: loss 2915.758475\n",
            "iteration 600 / 1500: loss 2894.814441\n",
            "iteration 700 / 1500: loss 2874.686679\n",
            "iteration 800 / 1500: loss 2859.166205\n",
            "iteration 900 / 1500: loss 2843.192282\n",
            "iteration 1000 / 1500: loss 2827.747562\n",
            "iteration 1100 / 1500: loss 2814.046552\n",
            "iteration 1200 / 1500: loss 2801.025192\n",
            "iteration 1300 / 1500: loss 2788.892665\n",
            "iteration 1400 / 1500: loss 2776.913057\n",
            "Grid_search:61\n",
            "iteration 0 / 1500: loss 27.480412\n",
            "iteration 100 / 1500: loss 10.513100\n",
            "iteration 200 / 1500: loss 8.985478\n",
            "iteration 300 / 1500: loss 9.958492\n",
            "iteration 400 / 1500: loss 8.403812\n",
            "iteration 500 / 1500: loss 8.624735\n",
            "iteration 600 / 1500: loss 8.768188\n",
            "iteration 700 / 1500: loss 8.798373\n",
            "iteration 800 / 1500: loss 8.865110\n",
            "iteration 900 / 1500: loss 9.308306\n",
            "iteration 1000 / 1500: loss 7.804422\n",
            "iteration 1100 / 1500: loss 7.590641\n",
            "iteration 1200 / 1500: loss 8.641278\n",
            "iteration 1300 / 1500: loss 8.015337\n",
            "iteration 1400 / 1500: loss 7.722911\n",
            "Grid_search:62\n",
            "iteration 0 / 1500: loss 29.182737\n",
            "iteration 100 / 1500: loss 14.426556\n",
            "iteration 200 / 1500: loss 13.189204\n",
            "iteration 300 / 1500: loss 12.629113\n",
            "iteration 400 / 1500: loss 12.361325\n",
            "iteration 500 / 1500: loss 12.357528\n",
            "iteration 600 / 1500: loss 12.814832\n",
            "iteration 700 / 1500: loss 11.428523\n",
            "iteration 800 / 1500: loss 10.927699\n",
            "iteration 900 / 1500: loss 11.823597\n",
            "iteration 1000 / 1500: loss 11.176545\n",
            "iteration 1100 / 1500: loss 11.330511\n",
            "iteration 1200 / 1500: loss 10.557460\n",
            "iteration 1300 / 1500: loss 10.503698\n",
            "iteration 1400 / 1500: loss 10.696988\n",
            "Grid_search:63\n",
            "iteration 0 / 1500: loss 36.196900\n",
            "iteration 100 / 1500: loss 21.217747\n",
            "iteration 200 / 1500: loss 19.876178\n",
            "iteration 300 / 1500: loss 19.412798\n",
            "iteration 400 / 1500: loss 19.236653\n",
            "iteration 500 / 1500: loss 19.434417\n",
            "iteration 600 / 1500: loss 18.969470\n",
            "iteration 700 / 1500: loss 19.611610\n",
            "iteration 800 / 1500: loss 17.803391\n",
            "iteration 900 / 1500: loss 18.530476\n",
            "iteration 1000 / 1500: loss 18.379899\n",
            "iteration 1100 / 1500: loss 18.212247\n",
            "iteration 1200 / 1500: loss 18.063716\n",
            "iteration 1300 / 1500: loss 17.785513\n",
            "iteration 1400 / 1500: loss 18.081625\n",
            "Grid_search:64\n",
            "iteration 0 / 1500: loss 48.464139\n",
            "iteration 100 / 1500: loss 37.891115\n",
            "iteration 200 / 1500: loss 37.544462\n",
            "iteration 300 / 1500: loss 35.832130\n",
            "iteration 400 / 1500: loss 34.909820\n",
            "iteration 500 / 1500: loss 35.190452\n",
            "iteration 600 / 1500: loss 33.865075\n",
            "iteration 700 / 1500: loss 33.850863\n",
            "iteration 800 / 1500: loss 33.141315\n",
            "iteration 900 / 1500: loss 33.433665\n",
            "iteration 1000 / 1500: loss 32.945992\n",
            "iteration 1100 / 1500: loss 32.221828\n",
            "iteration 1200 / 1500: loss 33.012381\n",
            "iteration 1300 / 1500: loss 32.413769\n",
            "iteration 1400 / 1500: loss 32.970491\n",
            "Grid_search:65\n",
            "iteration 0 / 1500: loss 86.807355\n",
            "iteration 100 / 1500: loss 72.659872\n",
            "iteration 200 / 1500: loss 71.002661\n",
            "iteration 300 / 1500: loss 68.354155\n",
            "iteration 400 / 1500: loss 68.075025\n",
            "iteration 500 / 1500: loss 67.349482\n",
            "iteration 600 / 1500: loss 66.093631\n",
            "iteration 700 / 1500: loss 66.750210\n",
            "iteration 800 / 1500: loss 64.967867\n",
            "iteration 900 / 1500: loss 65.665305\n",
            "iteration 1000 / 1500: loss 64.524491\n",
            "iteration 1100 / 1500: loss 64.225899\n",
            "iteration 1200 / 1500: loss 64.387494\n",
            "iteration 1300 / 1500: loss 64.435744\n",
            "iteration 1400 / 1500: loss 64.272497\n",
            "Grid_search:66\n",
            "iteration 0 / 1500: loss 164.096645\n",
            "iteration 100 / 1500: loss 150.068496\n",
            "iteration 200 / 1500: loss 146.186271\n",
            "iteration 300 / 1500: loss 143.788807\n",
            "iteration 400 / 1500: loss 142.130811\n",
            "iteration 500 / 1500: loss 139.992870\n",
            "iteration 600 / 1500: loss 140.177003\n",
            "iteration 700 / 1500: loss 137.859692\n",
            "iteration 800 / 1500: loss 137.455497\n",
            "iteration 900 / 1500: loss 136.784840\n",
            "iteration 1000 / 1500: loss 135.372947\n",
            "iteration 1100 / 1500: loss 135.387350\n",
            "iteration 1200 / 1500: loss 133.454332\n",
            "iteration 1300 / 1500: loss 133.813700\n",
            "iteration 1400 / 1500: loss 132.693407\n",
            "Grid_search:67\n",
            "iteration 0 / 1500: loss 330.723329\n",
            "iteration 100 / 1500: loss 306.141419\n",
            "iteration 200 / 1500: loss 301.220317\n",
            "iteration 300 / 1500: loss 298.484646\n",
            "iteration 400 / 1500: loss 295.214254\n",
            "iteration 500 / 1500: loss 291.475861\n",
            "iteration 600 / 1500: loss 289.694713\n",
            "iteration 700 / 1500: loss 287.379383\n",
            "iteration 800 / 1500: loss 285.090102\n",
            "iteration 900 / 1500: loss 283.707560\n",
            "iteration 1000 / 1500: loss 281.254534\n",
            "iteration 1100 / 1500: loss 279.614252\n",
            "iteration 1200 / 1500: loss 278.063964\n",
            "iteration 1300 / 1500: loss 276.578812\n",
            "iteration 1400 / 1500: loss 275.407327\n",
            "Grid_search:68\n",
            "iteration 0 / 1500: loss 685.267720\n",
            "iteration 100 / 1500: loss 654.952561\n",
            "iteration 200 / 1500: loss 645.184552\n",
            "iteration 300 / 1500: loss 637.680738\n",
            "iteration 400 / 1500: loss 629.991733\n",
            "iteration 500 / 1500: loss 624.269559\n",
            "iteration 600 / 1500: loss 618.387334\n",
            "iteration 700 / 1500: loss 614.097656\n",
            "iteration 800 / 1500: loss 610.347881\n",
            "iteration 900 / 1500: loss 606.010409\n",
            "iteration 1000 / 1500: loss 602.749184\n",
            "iteration 1100 / 1500: loss 600.024660\n",
            "iteration 1200 / 1500: loss 596.506738\n",
            "iteration 1300 / 1500: loss 593.989468\n",
            "iteration 1400 / 1500: loss 591.283740\n",
            "Grid_search:69\n",
            "iteration 0 / 1500: loss 1466.468293\n",
            "iteration 100 / 1500: loss 1423.806488\n",
            "iteration 200 / 1500: loss 1400.870241\n",
            "iteration 300 / 1500: loss 1384.975235\n",
            "iteration 400 / 1500: loss 1371.396301\n",
            "iteration 500 / 1500: loss 1357.567306\n",
            "iteration 600 / 1500: loss 1347.096390\n",
            "iteration 700 / 1500: loss 1336.850398\n",
            "iteration 800 / 1500: loss 1326.483957\n",
            "iteration 900 / 1500: loss 1319.119008\n",
            "iteration 1000 / 1500: loss 1311.998543\n",
            "iteration 1100 / 1500: loss 1305.508698\n",
            "iteration 1200 / 1500: loss 1297.199947\n",
            "iteration 1300 / 1500: loss 1290.862240\n",
            "iteration 1400 / 1500: loss 1286.228130\n",
            "Grid_search:70\n",
            "iteration 0 / 1500: loss 3096.508362\n",
            "iteration 100 / 1500: loss 3011.920919\n",
            "iteration 200 / 1500: loss 2965.854531\n",
            "iteration 300 / 1500: loss 2929.941135\n",
            "iteration 400 / 1500: loss 2899.461849\n",
            "iteration 500 / 1500: loss 2872.982486\n",
            "iteration 600 / 1500: loss 2850.656051\n",
            "iteration 700 / 1500: loss 2828.129741\n",
            "iteration 800 / 1500: loss 2810.261009\n",
            "iteration 900 / 1500: loss 2791.552528\n",
            "iteration 1000 / 1500: loss 2776.024357\n",
            "iteration 1100 / 1500: loss 2759.818334\n",
            "iteration 1200 / 1500: loss 2745.145286\n",
            "iteration 1300 / 1500: loss 2731.654843\n",
            "iteration 1400 / 1500: loss 2719.480828\n",
            "Grid_search:71\n",
            "iteration 0 / 1500: loss 23.677074\n",
            "iteration 100 / 1500: loss 10.546302\n",
            "iteration 200 / 1500: loss 8.662175\n",
            "iteration 300 / 1500: loss 8.429891\n",
            "iteration 400 / 1500: loss 9.179601\n",
            "iteration 500 / 1500: loss 8.400869\n",
            "iteration 600 / 1500: loss 8.013726\n",
            "iteration 700 / 1500: loss 8.399823\n",
            "iteration 800 / 1500: loss 7.735863\n",
            "iteration 900 / 1500: loss 8.622875\n",
            "iteration 1000 / 1500: loss 8.348021\n",
            "iteration 1100 / 1500: loss 8.692822\n",
            "iteration 1200 / 1500: loss 7.809375\n",
            "iteration 1300 / 1500: loss 7.826768\n",
            "iteration 1400 / 1500: loss 8.176708\n",
            "Grid_search:72\n",
            "iteration 0 / 1500: loss 30.031132\n",
            "iteration 100 / 1500: loss 13.860453\n",
            "iteration 200 / 1500: loss 12.972807\n",
            "iteration 300 / 1500: loss 12.907091\n",
            "iteration 400 / 1500: loss 12.013657\n",
            "iteration 500 / 1500: loss 12.462542\n",
            "iteration 600 / 1500: loss 11.750157\n",
            "iteration 700 / 1500: loss 10.866683\n",
            "iteration 800 / 1500: loss 11.515047\n",
            "iteration 900 / 1500: loss 11.404576\n",
            "iteration 1000 / 1500: loss 11.073745\n",
            "iteration 1100 / 1500: loss 11.615037\n",
            "iteration 1200 / 1500: loss 11.391591\n",
            "iteration 1300 / 1500: loss 11.052329\n",
            "iteration 1400 / 1500: loss 10.724552\n",
            "Grid_search:73\n",
            "iteration 0 / 1500: loss 35.434487\n",
            "iteration 100 / 1500: loss 20.623602\n",
            "iteration 200 / 1500: loss 20.002168\n",
            "iteration 300 / 1500: loss 20.372434\n",
            "iteration 400 / 1500: loss 19.447876\n",
            "iteration 500 / 1500: loss 18.897436\n",
            "iteration 600 / 1500: loss 18.969626\n",
            "iteration 700 / 1500: loss 19.303947\n",
            "iteration 800 / 1500: loss 18.388978\n",
            "iteration 900 / 1500: loss 18.501809\n",
            "iteration 1000 / 1500: loss 18.249103\n",
            "iteration 1100 / 1500: loss 17.995979\n",
            "iteration 1200 / 1500: loss 17.560970\n",
            "iteration 1300 / 1500: loss 17.236785\n",
            "iteration 1400 / 1500: loss 17.200303\n",
            "Grid_search:74\n",
            "iteration 0 / 1500: loss 52.112346\n",
            "iteration 100 / 1500: loss 38.132036\n",
            "iteration 200 / 1500: loss 35.791537\n",
            "iteration 300 / 1500: loss 35.846790\n",
            "iteration 400 / 1500: loss 34.806572\n",
            "iteration 500 / 1500: loss 33.783683\n",
            "iteration 600 / 1500: loss 33.996902\n",
            "iteration 700 / 1500: loss 32.735971\n",
            "iteration 800 / 1500: loss 33.223919\n",
            "iteration 900 / 1500: loss 33.735355\n",
            "iteration 1000 / 1500: loss 33.516084\n",
            "iteration 1100 / 1500: loss 32.244026\n",
            "iteration 1200 / 1500: loss 31.562802\n",
            "iteration 1300 / 1500: loss 31.496571\n",
            "iteration 1400 / 1500: loss 31.405035\n",
            "Grid_search:75\n",
            "iteration 0 / 1500: loss 86.573430\n",
            "iteration 100 / 1500: loss 72.740448\n",
            "iteration 200 / 1500: loss 69.627408\n",
            "iteration 300 / 1500: loss 68.444833\n",
            "iteration 400 / 1500: loss 67.714337\n",
            "iteration 500 / 1500: loss 65.893634\n",
            "iteration 600 / 1500: loss 66.557771\n",
            "iteration 700 / 1500: loss 66.808847\n",
            "iteration 800 / 1500: loss 65.244034\n",
            "iteration 900 / 1500: loss 65.141023\n",
            "iteration 1000 / 1500: loss 65.184413\n",
            "iteration 1100 / 1500: loss 63.775889\n",
            "iteration 1200 / 1500: loss 64.013017\n",
            "iteration 1300 / 1500: loss 63.587973\n",
            "iteration 1400 / 1500: loss 63.065232\n",
            "Grid_search:76\n",
            "iteration 0 / 1500: loss 165.308418\n",
            "iteration 100 / 1500: loss 146.154860\n",
            "iteration 200 / 1500: loss 141.885085\n",
            "iteration 300 / 1500: loss 141.020283\n",
            "iteration 400 / 1500: loss 139.026760\n",
            "iteration 500 / 1500: loss 137.188920\n",
            "iteration 600 / 1500: loss 136.259290\n",
            "iteration 700 / 1500: loss 134.542524\n",
            "iteration 800 / 1500: loss 133.393309\n",
            "iteration 900 / 1500: loss 133.581217\n",
            "iteration 1000 / 1500: loss 131.915782\n",
            "iteration 1100 / 1500: loss 132.391784\n",
            "iteration 1200 / 1500: loss 131.199892\n",
            "iteration 1300 / 1500: loss 130.407695\n",
            "iteration 1400 / 1500: loss 129.601511\n",
            "Grid_search:77\n",
            "iteration 0 / 1500: loss 330.991030\n",
            "iteration 100 / 1500: loss 301.983075\n",
            "iteration 200 / 1500: loss 296.641895\n",
            "iteration 300 / 1500: loss 291.952932\n",
            "iteration 400 / 1500: loss 288.515106\n",
            "iteration 500 / 1500: loss 285.241961\n",
            "iteration 600 / 1500: loss 283.977585\n",
            "iteration 700 / 1500: loss 281.081712\n",
            "iteration 800 / 1500: loss 279.610164\n",
            "iteration 900 / 1500: loss 277.417973\n",
            "iteration 1000 / 1500: loss 275.154560\n",
            "iteration 1100 / 1500: loss 273.834764\n",
            "iteration 1200 / 1500: loss 272.848486\n",
            "iteration 1300 / 1500: loss 271.914844\n",
            "iteration 1400 / 1500: loss 269.930698\n",
            "Grid_search:78\n",
            "iteration 0 / 1500: loss 678.751175\n",
            "iteration 100 / 1500: loss 648.682836\n",
            "iteration 200 / 1500: loss 636.908788\n",
            "iteration 300 / 1500: loss 629.557250\n",
            "iteration 400 / 1500: loss 622.306347\n",
            "iteration 500 / 1500: loss 615.104715\n",
            "iteration 600 / 1500: loss 610.491011\n",
            "iteration 700 / 1500: loss 605.684377\n",
            "iteration 800 / 1500: loss 601.727831\n",
            "iteration 900 / 1500: loss 597.309333\n",
            "iteration 1000 / 1500: loss 593.532181\n",
            "iteration 1100 / 1500: loss 589.963527\n",
            "iteration 1200 / 1500: loss 587.249673\n",
            "iteration 1300 / 1500: loss 585.706881\n",
            "iteration 1400 / 1500: loss 582.980959\n",
            "Grid_search:79\n",
            "iteration 0 / 1500: loss 1472.925670\n",
            "iteration 100 / 1500: loss 1414.743608\n",
            "iteration 200 / 1500: loss 1390.365952\n",
            "iteration 300 / 1500: loss 1372.568472\n",
            "iteration 400 / 1500: loss 1355.160315\n",
            "iteration 500 / 1500: loss 1342.527981\n",
            "iteration 600 / 1500: loss 1330.212459\n",
            "iteration 700 / 1500: loss 1319.068760\n",
            "iteration 800 / 1500: loss 1310.473169\n",
            "iteration 900 / 1500: loss 1301.405818\n",
            "iteration 1000 / 1500: loss 1293.934245\n",
            "iteration 1100 / 1500: loss 1286.327568\n",
            "iteration 1200 / 1500: loss 1280.947423\n",
            "iteration 1300 / 1500: loss 1274.038440\n",
            "iteration 1400 / 1500: loss 1267.105597\n",
            "Grid_search:80\n",
            "iteration 0 / 1500: loss 3120.376773\n",
            "iteration 100 / 1500: loss 3012.801431\n",
            "iteration 200 / 1500: loss 2960.022063\n",
            "iteration 300 / 1500: loss 2919.055130\n",
            "iteration 400 / 1500: loss 2885.122722\n",
            "iteration 500 / 1500: loss 2858.831062\n",
            "iteration 600 / 1500: loss 2832.639660\n",
            "iteration 700 / 1500: loss 2811.757256\n",
            "iteration 800 / 1500: loss 2791.799575\n",
            "iteration 900 / 1500: loss 2773.623381\n",
            "iteration 1000 / 1500: loss 2756.692909\n",
            "iteration 1100 / 1500: loss 2741.232553\n",
            "iteration 1200 / 1500: loss 2726.468841\n",
            "iteration 1300 / 1500: loss 2714.974540\n",
            "iteration 1400 / 1500: loss 2702.782375\n",
            "Grid_search:81\n",
            "iteration 0 / 1500: loss 24.398180\n",
            "iteration 100 / 1500: loss 11.570776\n",
            "iteration 200 / 1500: loss 9.527422\n",
            "iteration 300 / 1500: loss 10.171669\n",
            "iteration 400 / 1500: loss 8.270868\n",
            "iteration 500 / 1500: loss 8.981640\n",
            "iteration 600 / 1500: loss 8.033471\n",
            "iteration 700 / 1500: loss 9.399056\n",
            "iteration 800 / 1500: loss 8.678201\n",
            "iteration 900 / 1500: loss 8.407947\n",
            "iteration 1000 / 1500: loss 7.297298\n",
            "iteration 1100 / 1500: loss 7.867664\n",
            "iteration 1200 / 1500: loss 7.841257\n",
            "iteration 1300 / 1500: loss 7.916003\n",
            "iteration 1400 / 1500: loss 8.654050\n",
            "Grid_search:82\n",
            "iteration 0 / 1500: loss 29.515591\n",
            "iteration 100 / 1500: loss 12.632254\n",
            "iteration 200 / 1500: loss 13.650623\n",
            "iteration 300 / 1500: loss 13.345385\n",
            "iteration 400 / 1500: loss 11.981504\n",
            "iteration 500 / 1500: loss 12.379362\n",
            "iteration 600 / 1500: loss 11.750998\n",
            "iteration 700 / 1500: loss 11.301438\n",
            "iteration 800 / 1500: loss 10.448839\n",
            "iteration 900 / 1500: loss 11.148559\n",
            "iteration 1000 / 1500: loss 10.997757\n",
            "iteration 1100 / 1500: loss 11.803492\n",
            "iteration 1200 / 1500: loss 9.807263\n",
            "iteration 1300 / 1500: loss 10.245329\n",
            "iteration 1400 / 1500: loss 10.760390\n",
            "Grid_search:83\n",
            "iteration 0 / 1500: loss 37.744282\n",
            "iteration 100 / 1500: loss 21.128751\n",
            "iteration 200 / 1500: loss 20.459395\n",
            "iteration 300 / 1500: loss 18.810528\n",
            "iteration 400 / 1500: loss 19.873360\n",
            "iteration 500 / 1500: loss 18.713934\n",
            "iteration 600 / 1500: loss 18.064942\n",
            "iteration 700 / 1500: loss 17.696149\n",
            "iteration 800 / 1500: loss 18.438160\n",
            "iteration 900 / 1500: loss 18.186771\n",
            "iteration 1000 / 1500: loss 18.713524\n",
            "iteration 1100 / 1500: loss 17.039774\n",
            "iteration 1200 / 1500: loss 17.587556\n",
            "iteration 1300 / 1500: loss 17.453821\n",
            "iteration 1400 / 1500: loss 17.157719\n",
            "Grid_search:84\n",
            "iteration 0 / 1500: loss 50.029175\n",
            "iteration 100 / 1500: loss 37.009183\n",
            "iteration 200 / 1500: loss 35.990825\n",
            "iteration 300 / 1500: loss 34.416792\n",
            "iteration 400 / 1500: loss 35.005450\n",
            "iteration 500 / 1500: loss 33.783555\n",
            "iteration 600 / 1500: loss 33.559876\n",
            "iteration 700 / 1500: loss 32.488960\n",
            "iteration 800 / 1500: loss 32.449765\n",
            "iteration 900 / 1500: loss 34.137530\n",
            "iteration 1000 / 1500: loss 33.128926\n",
            "iteration 1100 / 1500: loss 32.870029\n",
            "iteration 1200 / 1500: loss 31.833906\n",
            "iteration 1300 / 1500: loss 33.710578\n",
            "iteration 1400 / 1500: loss 32.509444\n",
            "Grid_search:85\n",
            "iteration 0 / 1500: loss 87.413329\n",
            "iteration 100 / 1500: loss 71.698047\n",
            "iteration 200 / 1500: loss 69.891056\n",
            "iteration 300 / 1500: loss 68.644667\n",
            "iteration 400 / 1500: loss 67.064867\n",
            "iteration 500 / 1500: loss 66.680156\n",
            "iteration 600 / 1500: loss 65.967273\n",
            "iteration 700 / 1500: loss 66.373854\n",
            "iteration 800 / 1500: loss 65.040102\n",
            "iteration 900 / 1500: loss 64.761147\n",
            "iteration 1000 / 1500: loss 63.574406\n",
            "iteration 1100 / 1500: loss 63.445975\n",
            "iteration 1200 / 1500: loss 62.136738\n",
            "iteration 1300 / 1500: loss 62.716973\n",
            "iteration 1400 / 1500: loss 63.037522\n",
            "Grid_search:86\n",
            "iteration 0 / 1500: loss 163.204429\n",
            "iteration 100 / 1500: loss 146.264589\n",
            "iteration 200 / 1500: loss 143.062731\n",
            "iteration 300 / 1500: loss 140.112889\n",
            "iteration 400 / 1500: loss 138.833235\n",
            "iteration 500 / 1500: loss 137.551927\n",
            "iteration 600 / 1500: loss 135.958114\n",
            "iteration 700 / 1500: loss 134.820463\n",
            "iteration 800 / 1500: loss 134.304780\n",
            "iteration 900 / 1500: loss 134.481248\n",
            "iteration 1000 / 1500: loss 132.157904\n",
            "iteration 1100 / 1500: loss 132.483457\n",
            "iteration 1200 / 1500: loss 131.945814\n",
            "iteration 1300 / 1500: loss 130.634952\n",
            "iteration 1400 / 1500: loss 130.289011\n",
            "Grid_search:87\n",
            "iteration 0 / 1500: loss 328.917324\n",
            "iteration 100 / 1500: loss 304.370410\n",
            "iteration 200 / 1500: loss 299.039220\n",
            "iteration 300 / 1500: loss 295.094353\n",
            "iteration 400 / 1500: loss 290.825520\n",
            "iteration 500 / 1500: loss 288.924285\n",
            "iteration 600 / 1500: loss 285.457296\n",
            "iteration 700 / 1500: loss 283.553646\n",
            "iteration 800 / 1500: loss 280.404890\n",
            "iteration 900 / 1500: loss 278.932039\n",
            "iteration 1000 / 1500: loss 278.000632\n",
            "iteration 1100 / 1500: loss 276.395553\n",
            "iteration 1200 / 1500: loss 275.660912\n",
            "iteration 1300 / 1500: loss 274.731356\n",
            "iteration 1400 / 1500: loss 274.154261\n",
            "Grid_search:88\n",
            "iteration 0 / 1500: loss 688.580488\n",
            "iteration 100 / 1500: loss 654.870096\n",
            "iteration 200 / 1500: loss 641.391651\n",
            "iteration 300 / 1500: loss 632.851864\n",
            "iteration 400 / 1500: loss 624.351639\n",
            "iteration 500 / 1500: loss 619.038061\n",
            "iteration 600 / 1500: loss 612.482158\n",
            "iteration 700 / 1500: loss 608.794022\n",
            "iteration 800 / 1500: loss 604.099755\n",
            "iteration 900 / 1500: loss 600.595720\n",
            "iteration 1000 / 1500: loss 597.845997\n",
            "iteration 1100 / 1500: loss 594.803395\n",
            "iteration 1200 / 1500: loss 591.886774\n",
            "iteration 1300 / 1500: loss 589.411076\n",
            "iteration 1400 / 1500: loss 586.670478\n",
            "Grid_search:89\n",
            "iteration 0 / 1500: loss 1477.051776\n",
            "iteration 100 / 1500: loss 1407.896129\n",
            "iteration 200 / 1500: loss 1379.034974\n",
            "iteration 300 / 1500: loss 1355.990059\n",
            "iteration 400 / 1500: loss 1341.235803\n",
            "iteration 500 / 1500: loss 1325.761912\n",
            "iteration 600 / 1500: loss 1315.400715\n",
            "iteration 700 / 1500: loss 1302.654055\n",
            "iteration 800 / 1500: loss 1294.571278\n",
            "iteration 900 / 1500: loss 1286.714045\n",
            "iteration 1000 / 1500: loss 1278.668384\n",
            "iteration 1100 / 1500: loss 1273.467859\n",
            "iteration 1200 / 1500: loss 1268.310265\n",
            "iteration 1300 / 1500: loss 1263.656329\n",
            "iteration 1400 / 1500: loss 1258.533706\n",
            "Grid_search:90\n",
            "iteration 0 / 1500: loss 3075.190857\n",
            "iteration 100 / 1500: loss 2955.336476\n",
            "iteration 200 / 1500: loss 2898.411282\n",
            "iteration 300 / 1500: loss 2854.318650\n",
            "iteration 400 / 1500: loss 2821.414887\n",
            "iteration 500 / 1500: loss 2792.371523\n",
            "iteration 600 / 1500: loss 2767.287826\n",
            "iteration 700 / 1500: loss 2748.254871\n",
            "iteration 800 / 1500: loss 2728.447416\n",
            "iteration 900 / 1500: loss 2713.554925\n",
            "iteration 1000 / 1500: loss 2699.163074\n",
            "iteration 1100 / 1500: loss 2687.152671\n",
            "iteration 1200 / 1500: loss 2675.295678\n",
            "iteration 1300 / 1500: loss 2666.145722\n",
            "iteration 1400 / 1500: loss 2657.216301\n",
            "Grid_search:91\n",
            "iteration 0 / 1500: loss 22.636033\n",
            "iteration 100 / 1500: loss 11.178885\n",
            "iteration 200 / 1500: loss 10.439790\n",
            "iteration 300 / 1500: loss 9.851374\n",
            "iteration 400 / 1500: loss 9.484675\n",
            "iteration 500 / 1500: loss 10.936247\n",
            "iteration 600 / 1500: loss 8.734290\n",
            "iteration 700 / 1500: loss 8.543180\n",
            "iteration 800 / 1500: loss 9.076309\n",
            "iteration 900 / 1500: loss 8.883641\n",
            "iteration 1000 / 1500: loss 8.664282\n",
            "iteration 1100 / 1500: loss 8.967419\n",
            "iteration 1200 / 1500: loss 7.899353\n",
            "iteration 1300 / 1500: loss 7.443768\n",
            "iteration 1400 / 1500: loss 8.300031\n",
            "Grid_search:92\n",
            "iteration 0 / 1500: loss 33.885707\n",
            "iteration 100 / 1500: loss 12.667336\n",
            "iteration 200 / 1500: loss 14.034126\n",
            "iteration 300 / 1500: loss 15.286349\n",
            "iteration 400 / 1500: loss 13.713455\n",
            "iteration 500 / 1500: loss 12.458452\n",
            "iteration 600 / 1500: loss 12.823995\n",
            "iteration 700 / 1500: loss 13.638961\n",
            "iteration 800 / 1500: loss 13.230477\n",
            "iteration 900 / 1500: loss 11.225235\n",
            "iteration 1000 / 1500: loss 12.991111\n",
            "iteration 1100 / 1500: loss 12.382340\n",
            "iteration 1200 / 1500: loss 15.085145\n",
            "iteration 1300 / 1500: loss 11.219847\n",
            "iteration 1400 / 1500: loss 11.322119\n",
            "Grid_search:93\n",
            "iteration 0 / 1500: loss 35.768221\n",
            "iteration 100 / 1500: loss 20.345529\n",
            "iteration 200 / 1500: loss 21.086003\n",
            "iteration 300 / 1500: loss 19.295814\n",
            "iteration 400 / 1500: loss 19.213667\n",
            "iteration 500 / 1500: loss 20.138031\n",
            "iteration 600 / 1500: loss 20.650031\n",
            "iteration 700 / 1500: loss 19.904129\n",
            "iteration 800 / 1500: loss 19.638648\n",
            "iteration 900 / 1500: loss 19.631840\n",
            "iteration 1000 / 1500: loss 17.504287\n",
            "iteration 1100 / 1500: loss 17.570264\n",
            "iteration 1200 / 1500: loss 18.820050\n",
            "iteration 1300 / 1500: loss 17.932032\n",
            "iteration 1400 / 1500: loss 19.042649\n",
            "Grid_search:94\n",
            "iteration 0 / 1500: loss 55.615861\n",
            "iteration 100 / 1500: loss 37.878610\n",
            "iteration 200 / 1500: loss 36.365896\n",
            "iteration 300 / 1500: loss 35.194159\n",
            "iteration 400 / 1500: loss 34.225285\n",
            "iteration 500 / 1500: loss 34.408326\n",
            "iteration 600 / 1500: loss 34.873905\n",
            "iteration 700 / 1500: loss 33.524609\n",
            "iteration 800 / 1500: loss 34.227944\n",
            "iteration 900 / 1500: loss 33.314939\n",
            "iteration 1000 / 1500: loss 35.713850\n",
            "iteration 1100 / 1500: loss 33.603768\n",
            "iteration 1200 / 1500: loss 34.035231\n",
            "iteration 1300 / 1500: loss 33.678777\n",
            "iteration 1400 / 1500: loss 33.912353\n",
            "Grid_search:95\n",
            "iteration 0 / 1500: loss 91.546827\n",
            "iteration 100 / 1500: loss 70.960164\n",
            "iteration 200 / 1500: loss 69.289804\n",
            "iteration 300 / 1500: loss 67.912625\n",
            "iteration 400 / 1500: loss 67.864303\n",
            "iteration 500 / 1500: loss 66.498116\n",
            "iteration 600 / 1500: loss 66.310097\n",
            "iteration 700 / 1500: loss 65.142597\n",
            "iteration 800 / 1500: loss 65.652895\n",
            "iteration 900 / 1500: loss 64.557376\n",
            "iteration 1000 / 1500: loss 64.149555\n",
            "iteration 1100 / 1500: loss 64.767368\n",
            "iteration 1200 / 1500: loss 64.040813\n",
            "iteration 1300 / 1500: loss 64.189517\n",
            "iteration 1400 / 1500: loss 64.520424\n",
            "Grid_search:96\n",
            "iteration 0 / 1500: loss 165.980108\n",
            "iteration 100 / 1500: loss 147.162333\n",
            "iteration 200 / 1500: loss 143.337117\n",
            "iteration 300 / 1500: loss 140.661139\n",
            "iteration 400 / 1500: loss 139.064985\n",
            "iteration 500 / 1500: loss 137.023823\n",
            "iteration 600 / 1500: loss 135.306089\n",
            "iteration 700 / 1500: loss 135.511980\n",
            "iteration 800 / 1500: loss 134.542940\n",
            "iteration 900 / 1500: loss 133.578315\n",
            "iteration 1000 / 1500: loss 132.542722\n",
            "iteration 1100 / 1500: loss 133.592434\n",
            "iteration 1200 / 1500: loss 132.400419\n",
            "iteration 1300 / 1500: loss 132.290807\n",
            "iteration 1400 / 1500: loss 132.205612\n",
            "Grid_search:97\n",
            "iteration 0 / 1500: loss 328.002586\n",
            "iteration 100 / 1500: loss 303.259728\n",
            "iteration 200 / 1500: loss 297.657180\n",
            "iteration 300 / 1500: loss 293.100180\n",
            "iteration 400 / 1500: loss 289.735780\n",
            "iteration 500 / 1500: loss 286.868499\n",
            "iteration 600 / 1500: loss 285.351924\n",
            "iteration 700 / 1500: loss 283.041863\n",
            "iteration 800 / 1500: loss 282.380591\n",
            "iteration 900 / 1500: loss 281.486605\n",
            "iteration 1000 / 1500: loss 279.363872\n",
            "iteration 1100 / 1500: loss 279.031487\n",
            "iteration 1200 / 1500: loss 278.942919\n",
            "iteration 1300 / 1500: loss 276.748608\n",
            "iteration 1400 / 1500: loss 278.184933\n",
            "Grid_search:98\n",
            "iteration 0 / 1500: loss 686.963217\n",
            "iteration 100 / 1500: loss 645.880959\n",
            "iteration 200 / 1500: loss 631.116544\n",
            "iteration 300 / 1500: loss 622.293282\n",
            "iteration 400 / 1500: loss 614.852889\n",
            "iteration 500 / 1500: loss 609.818347\n",
            "iteration 600 / 1500: loss 603.698454\n",
            "iteration 700 / 1500: loss 600.257511\n",
            "iteration 800 / 1500: loss 598.511144\n",
            "iteration 900 / 1500: loss 594.765211\n",
            "iteration 1000 / 1500: loss 593.183207\n",
            "iteration 1100 / 1500: loss 590.548870\n",
            "iteration 1200 / 1500: loss 589.672403\n",
            "iteration 1300 / 1500: loss 588.834062\n",
            "iteration 1400 / 1500: loss 586.502119\n",
            "Grid_search:99\n",
            "iteration 0 / 1500: loss 1474.265097\n",
            "iteration 100 / 1500: loss 1406.289558\n",
            "iteration 200 / 1500: loss 1379.687820\n",
            "iteration 300 / 1500: loss 1355.532188\n",
            "iteration 400 / 1500: loss 1341.192372\n",
            "iteration 500 / 1500: loss 1327.219097\n",
            "iteration 600 / 1500: loss 1317.390520\n",
            "iteration 700 / 1500: loss 1308.232532\n",
            "iteration 800 / 1500: loss 1299.920084\n",
            "iteration 900 / 1500: loss 1295.948805\n",
            "iteration 1000 / 1500: loss 1287.394035\n",
            "iteration 1100 / 1500: loss 1285.822008\n",
            "iteration 1200 / 1500: loss 1280.003166\n",
            "iteration 1300 / 1500: loss 1276.441335\n",
            "iteration 1400 / 1500: loss 1273.961985\n",
            "Grid_search:100\n",
            "iteration 0 / 1500: loss 3086.172650\n",
            "iteration 100 / 1500: loss 2968.647614\n",
            "iteration 200 / 1500: loss 2914.831124\n",
            "iteration 300 / 1500: loss 2876.420805\n",
            "iteration 400 / 1500: loss 2845.620562\n",
            "iteration 500 / 1500: loss 2821.063652\n",
            "iteration 600 / 1500: loss 2804.669258\n",
            "iteration 700 / 1500: loss 2784.840284\n",
            "iteration 800 / 1500: loss 2771.031568\n",
            "iteration 900 / 1500: loss 2756.495662\n",
            "iteration 1000 / 1500: loss 2746.488274\n",
            "iteration 1100 / 1500: loss 2737.587063\n",
            "iteration 1200 / 1500: loss 2732.148000\n",
            "iteration 1300 / 1500: loss 2726.497109\n",
            "iteration 1400 / 1500: loss 2723.640417\n",
            "lr 7.943282e-08 reg 1.000000e+02 train accuracy: 0.293367 val accuracy: 0.313000\n",
            "lr 7.943282e-08 reg 2.154435e+02 train accuracy: 0.294020 val accuracy: 0.295000\n",
            "lr 7.943282e-08 reg 4.641589e+02 train accuracy: 0.298102 val accuracy: 0.301000\n",
            "lr 7.943282e-08 reg 1.000000e+03 train accuracy: 0.294694 val accuracy: 0.319000\n",
            "lr 7.943282e-08 reg 2.154435e+03 train accuracy: 0.289959 val accuracy: 0.309000\n",
            "lr 7.943282e-08 reg 4.641589e+03 train accuracy: 0.295857 val accuracy: 0.305000\n",
            "lr 7.943282e-08 reg 1.000000e+04 train accuracy: 0.300408 val accuracy: 0.308000\n",
            "lr 7.943282e-08 reg 2.154435e+04 train accuracy: 0.293878 val accuracy: 0.309000\n",
            "lr 7.943282e-08 reg 4.641589e+04 train accuracy: 0.292449 val accuracy: 0.295000\n",
            "lr 7.943282e-08 reg 1.000000e+05 train accuracy: 0.295755 val accuracy: 0.306000\n",
            "lr 1.136464e-07 reg 1.000000e+02 train accuracy: 0.302571 val accuracy: 0.321000\n",
            "lr 1.136464e-07 reg 2.154435e+02 train accuracy: 0.314000 val accuracy: 0.328000\n",
            "lr 1.136464e-07 reg 4.641589e+02 train accuracy: 0.306694 val accuracy: 0.319000\n",
            "lr 1.136464e-07 reg 1.000000e+03 train accuracy: 0.307204 val accuracy: 0.310000\n",
            "lr 1.136464e-07 reg 2.154435e+03 train accuracy: 0.306510 val accuracy: 0.312000\n",
            "lr 1.136464e-07 reg 4.641589e+03 train accuracy: 0.303694 val accuracy: 0.309000\n",
            "lr 1.136464e-07 reg 1.000000e+04 train accuracy: 0.307327 val accuracy: 0.292000\n",
            "lr 1.136464e-07 reg 2.154435e+04 train accuracy: 0.313082 val accuracy: 0.305000\n",
            "lr 1.136464e-07 reg 4.641589e+04 train accuracy: 0.305551 val accuracy: 0.300000\n",
            "lr 1.136464e-07 reg 1.000000e+05 train accuracy: 0.302633 val accuracy: 0.305000\n",
            "lr 1.625965e-07 reg 1.000000e+02 train accuracy: 0.318265 val accuracy: 0.329000\n",
            "lr 1.625965e-07 reg 2.154435e+02 train accuracy: 0.324408 val accuracy: 0.335000\n",
            "lr 1.625965e-07 reg 4.641589e+02 train accuracy: 0.324673 val accuracy: 0.318000\n",
            "lr 1.625965e-07 reg 1.000000e+03 train accuracy: 0.321286 val accuracy: 0.333000\n",
            "lr 1.625965e-07 reg 2.154435e+03 train accuracy: 0.323082 val accuracy: 0.334000\n",
            "lr 1.625965e-07 reg 4.641589e+03 train accuracy: 0.319163 val accuracy: 0.315000\n",
            "lr 1.625965e-07 reg 1.000000e+04 train accuracy: 0.318163 val accuracy: 0.317000\n",
            "lr 1.625965e-07 reg 2.154435e+04 train accuracy: 0.317469 val accuracy: 0.304000\n",
            "lr 1.625965e-07 reg 4.641589e+04 train accuracy: 0.320367 val accuracy: 0.332000\n",
            "lr 1.625965e-07 reg 1.000000e+05 train accuracy: 0.322714 val accuracy: 0.340000\n",
            "lr 2.326305e-07 reg 1.000000e+02 train accuracy: 0.334347 val accuracy: 0.351000\n",
            "lr 2.326305e-07 reg 2.154435e+02 train accuracy: 0.334837 val accuracy: 0.345000\n",
            "lr 2.326305e-07 reg 4.641589e+02 train accuracy: 0.328490 val accuracy: 0.309000\n",
            "lr 2.326305e-07 reg 1.000000e+03 train accuracy: 0.335020 val accuracy: 0.360000\n",
            "lr 2.326305e-07 reg 2.154435e+03 train accuracy: 0.327837 val accuracy: 0.323000\n",
            "lr 2.326305e-07 reg 4.641589e+03 train accuracy: 0.328143 val accuracy: 0.338000\n",
            "lr 2.326305e-07 reg 1.000000e+04 train accuracy: 0.331061 val accuracy: 0.334000\n",
            "lr 2.326305e-07 reg 2.154435e+04 train accuracy: 0.331388 val accuracy: 0.352000\n",
            "lr 2.326305e-07 reg 4.641589e+04 train accuracy: 0.329265 val accuracy: 0.333000\n",
            "lr 2.326305e-07 reg 1.000000e+05 train accuracy: 0.334571 val accuracy: 0.328000\n",
            "lr 3.328298e-07 reg 1.000000e+02 train accuracy: 0.341143 val accuracy: 0.351000\n",
            "lr 3.328298e-07 reg 2.154435e+02 train accuracy: 0.338245 val accuracy: 0.351000\n",
            "lr 3.328298e-07 reg 4.641589e+02 train accuracy: 0.343061 val accuracy: 0.370000\n",
            "lr 3.328298e-07 reg 1.000000e+03 train accuracy: 0.336898 val accuracy: 0.330000\n",
            "lr 3.328298e-07 reg 2.154435e+03 train accuracy: 0.342163 val accuracy: 0.330000\n",
            "lr 3.328298e-07 reg 4.641589e+03 train accuracy: 0.349061 val accuracy: 0.343000\n",
            "lr 3.328298e-07 reg 1.000000e+04 train accuracy: 0.340571 val accuracy: 0.347000\n",
            "lr 3.328298e-07 reg 2.154435e+04 train accuracy: 0.341510 val accuracy: 0.353000\n",
            "lr 3.328298e-07 reg 4.641589e+04 train accuracy: 0.347714 val accuracy: 0.339000\n",
            "lr 3.328298e-07 reg 1.000000e+05 train accuracy: 0.346469 val accuracy: 0.342000\n",
            "lr 4.761873e-07 reg 1.000000e+02 train accuracy: 0.350857 val accuracy: 0.335000\n",
            "lr 4.761873e-07 reg 2.154435e+02 train accuracy: 0.354327 val accuracy: 0.362000\n",
            "lr 4.761873e-07 reg 4.641589e+02 train accuracy: 0.350510 val accuracy: 0.351000\n",
            "lr 4.761873e-07 reg 1.000000e+03 train accuracy: 0.358163 val accuracy: 0.342000\n",
            "lr 4.761873e-07 reg 2.154435e+03 train accuracy: 0.347347 val accuracy: 0.341000\n",
            "lr 4.761873e-07 reg 4.641589e+03 train accuracy: 0.349490 val accuracy: 0.333000\n",
            "lr 4.761873e-07 reg 1.000000e+04 train accuracy: 0.358898 val accuracy: 0.352000\n",
            "lr 4.761873e-07 reg 2.154435e+04 train accuracy: 0.357347 val accuracy: 0.349000\n",
            "lr 4.761873e-07 reg 4.641589e+04 train accuracy: 0.357347 val accuracy: 0.351000\n",
            "lr 4.761873e-07 reg 1.000000e+05 train accuracy: 0.354184 val accuracy: 0.330000\n",
            "lr 6.812921e-07 reg 1.000000e+02 train accuracy: 0.361122 val accuracy: 0.361000\n",
            "lr 6.812921e-07 reg 2.154435e+02 train accuracy: 0.365612 val accuracy: 0.371000\n",
            "lr 6.812921e-07 reg 4.641589e+02 train accuracy: 0.361959 val accuracy: 0.358000\n",
            "lr 6.812921e-07 reg 1.000000e+03 train accuracy: 0.356449 val accuracy: 0.331000\n",
            "lr 6.812921e-07 reg 2.154435e+03 train accuracy: 0.353878 val accuracy: 0.371000\n",
            "lr 6.812921e-07 reg 4.641589e+03 train accuracy: 0.368898 val accuracy: 0.376000\n",
            "lr 6.812921e-07 reg 1.000000e+04 train accuracy: 0.346612 val accuracy: 0.359000\n",
            "lr 6.812921e-07 reg 2.154435e+04 train accuracy: 0.353388 val accuracy: 0.351000\n",
            "lr 6.812921e-07 reg 4.641589e+04 train accuracy: 0.362245 val accuracy: 0.348000\n",
            "lr 6.812921e-07 reg 1.000000e+05 train accuracy: 0.359041 val accuracy: 0.353000\n",
            "lr 9.747402e-07 reg 1.000000e+02 train accuracy: 0.359163 val accuracy: 0.354000\n",
            "lr 9.747402e-07 reg 2.154435e+02 train accuracy: 0.360184 val accuracy: 0.368000\n",
            "lr 9.747402e-07 reg 4.641589e+02 train accuracy: 0.366551 val accuracy: 0.360000\n",
            "lr 9.747402e-07 reg 1.000000e+03 train accuracy: 0.350714 val accuracy: 0.342000\n",
            "lr 9.747402e-07 reg 2.154435e+03 train accuracy: 0.361653 val accuracy: 0.335000\n",
            "lr 9.747402e-07 reg 4.641589e+03 train accuracy: 0.361061 val accuracy: 0.346000\n",
            "lr 9.747402e-07 reg 1.000000e+04 train accuracy: 0.348082 val accuracy: 0.337000\n",
            "lr 9.747402e-07 reg 2.154435e+04 train accuracy: 0.360429 val accuracy: 0.346000\n",
            "lr 9.747402e-07 reg 4.641589e+04 train accuracy: 0.357816 val accuracy: 0.351000\n",
            "lr 9.747402e-07 reg 1.000000e+05 train accuracy: 0.374000 val accuracy: 0.383000\n",
            "lr 1.394583e-06 reg 1.000000e+02 train accuracy: 0.375265 val accuracy: 0.387000\n",
            "lr 1.394583e-06 reg 2.154435e+02 train accuracy: 0.330837 val accuracy: 0.320000\n",
            "lr 1.394583e-06 reg 4.641589e+02 train accuracy: 0.358469 val accuracy: 0.356000\n",
            "lr 1.394583e-06 reg 1.000000e+03 train accuracy: 0.341918 val accuracy: 0.331000\n",
            "lr 1.394583e-06 reg 2.154435e+03 train accuracy: 0.339673 val accuracy: 0.351000\n",
            "lr 1.394583e-06 reg 4.641589e+03 train accuracy: 0.354592 val accuracy: 0.349000\n",
            "lr 1.394583e-06 reg 1.000000e+04 train accuracy: 0.365000 val accuracy: 0.341000\n",
            "lr 1.394583e-06 reg 2.154435e+04 train accuracy: 0.342143 val accuracy: 0.317000\n",
            "lr 1.394583e-06 reg 4.641589e+04 train accuracy: 0.362918 val accuracy: 0.369000\n",
            "lr 1.394583e-06 reg 1.000000e+05 train accuracy: 0.348102 val accuracy: 0.347000\n",
            "lr 1.995262e-06 reg 1.000000e+02 train accuracy: 0.349857 val accuracy: 0.330000\n",
            "lr 1.995262e-06 reg 2.154435e+02 train accuracy: 0.350510 val accuracy: 0.341000\n",
            "lr 1.995262e-06 reg 4.641589e+02 train accuracy: 0.349388 val accuracy: 0.341000\n",
            "lr 1.995262e-06 reg 1.000000e+03 train accuracy: 0.357673 val accuracy: 0.341000\n",
            "lr 1.995262e-06 reg 2.154435e+03 train accuracy: 0.370490 val accuracy: 0.363000\n",
            "lr 1.995262e-06 reg 4.641589e+03 train accuracy: 0.352918 val accuracy: 0.335000\n",
            "lr 1.995262e-06 reg 1.000000e+04 train accuracy: 0.356163 val accuracy: 0.374000\n",
            "lr 1.995262e-06 reg 2.154435e+04 train accuracy: 0.327531 val accuracy: 0.337000\n",
            "lr 1.995262e-06 reg 4.641589e+04 train accuracy: 0.360102 val accuracy: 0.347000\n",
            "lr 1.995262e-06 reg 1.000000e+05 train accuracy: 0.333408 val accuracy: 0.308000\n",
            "best validation accuracy achieved during cross-validation: 0.387000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huh-8lQARRUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "217c7fff-5e3a-4093-d8d4-ef3431ce01a7"
      },
      "source": [
        "learning_rates = np.logspace(-7.1, -5.7, num=10)\n",
        "regularization_strengths = np.logspace(2, 5, num=10)\n",
        "learning_rates"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.94328235e-08, 1.13646367e-07, 1.62596469e-07, 2.32630507e-07,\n",
              "       3.32829814e-07, 4.76187266e-07, 6.81292069e-07, 9.74740226e-07,\n",
              "       1.39458325e-06, 1.99526231e-06])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore-input"
        ],
        "id": "LTuie0UPGD2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the cross-validation results\n",
        "import math\n",
        "x_scatter = [math.log10(x[0]) for x in results]\n",
        "y_scatter = [math.log10(x[1]) for x in results]\n",
        "\n",
        "# plot training accuracy\n",
        "marker_size = 100\n",
        "colors = [results[x][0] for x in results]\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
        "plt.colorbar()\n",
        "plt.xlabel('log learning rate')\n",
        "plt.ylabel('log regularization strength')\n",
        "plt.title('CIFAR-10 training accuracy')\n",
        "\n",
        "# plot validation accuracy\n",
        "colors = [results[x][1] for x in results] # default size of markers is 20\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
        "plt.colorbar()\n",
        "plt.xlabel('log learning rate')\n",
        "plt.ylabel('log regularization strength')\n",
        "plt.title('CIFAR-10 validation accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orJwsw3fGD2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the best svm on test set\n",
        "y_test_pred = best_svm.predict(X_test)\n",
        "test_accuracy = np.mean(y_test == y_test_pred)\n",
        "print('linear SVM on raw pixels final test set accuracy: %f' % test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore-input"
        ],
        "id": "0pm8RpLSGD2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the learned weights for each class.\n",
        "# Depending on your choice of learning rate and regularization strength, these may\n",
        "# or may not be nice to look at.\n",
        "w = best_svm.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "      \n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-inline"
        ],
        "id": "MXgIekEQGD2f",
        "colab_type": "text"
      },
      "source": [
        "**Inline question 2**\n",
        "\n",
        "Describe what your visualized SVM weights look like, and offer a brief explanation for why they look they way that they do.\n",
        "\n",
        "$\\color{blue}{\\textit Your Answer:}$ *fill this in*  \n"
      ]
    }
  ]
}